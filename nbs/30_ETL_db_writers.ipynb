{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL DB writers\n",
    "\n",
    "> Pre-specified nodes that write data into the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp etl_db_writers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[01/19/25 19:29:08] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Using                                                                  <a href=\"file:///Users/magnus/miniforge3/envs/inventory_foundation/lib/python3.11/site-packages/kedro/framework/project/__init__.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">__init__.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/magnus/miniforge3/envs/inventory_foundation/lib/python3.11/site-packages/kedro/framework/project/__init__.py#270\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">270</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">'/Users/magnus/miniforge3/envs/inventory_foundation/lib/python3.11/sit</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">e-packages/kedro/framework/project/rich_logging.yml'</span> as logging        <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         configuration.                                                         <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[01/19/25 19:29:08]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Using                                                                  \u001b]8;id=307715;file:///Users/magnus/miniforge3/envs/inventory_foundation/lib/python3.11/site-packages/kedro/framework/project/__init__.py\u001b\\\u001b[2m__init__.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=590847;file:///Users/magnus/miniforge3/envs/inventory_foundation/lib/python3.11/site-packages/kedro/framework/project/__init__.py#270\u001b\\\u001b[2m270\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32m'/Users/magnus/miniforge3/envs/inventory_foundation/lib/python3.11/sit\u001b[0m \u001b[2m               \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32me-packages/kedro/framework/project/rich_logging.yml'\u001b[0m as logging        \u001b[2m               \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         configuration.                                                         \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| export\n",
    "\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import psycopg2.extras\n",
    "import typing as t\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import shutil\n",
    "\n",
    "from inventory_foundation_sdk.db_mgmt import get_db_credentials, insert_multi_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Company-level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def write_company_name(name: str, additional_info: t.Dict = None, ignore_company_if_exist: bool = True) -> int:\n",
    "     \n",
    "    \"\"\"\n",
    "    This function writes the company name to the database and any additional info.\n",
    "    Each key in `additional_info` becomes a column in the database table if it doesn't exist,\n",
    "    and the associated value is written to that column.\n",
    "    \n",
    "    If `ignore_company_if_exist` is False and the company name already exists, an error is raised.\n",
    "    If `ignore_company_if_exist` is True, a warning is logged and the existing record is updated if additional info differs.\n",
    "    \n",
    "    Returns the ID that the database has assigned to the company name.\n",
    "    \"\"\"\n",
    "\n",
    "    db_credentials = get_db_credentials()[\"con\"]\n",
    "\n",
    "    try:\n",
    "        with psycopg2.connect(db_credentials) as conn:\n",
    "            with conn.cursor() as cur:\n",
    "                # Try to insert the company name\n",
    "                cur.execute(\n",
    "                    \"\"\"\n",
    "                    INSERT INTO companies (name)\n",
    "                    VALUES (%s)\n",
    "                    ON CONFLICT (name) DO NOTHING\n",
    "                    RETURNING \"ID\";\n",
    "                    \"\"\",\n",
    "                    (name,)\n",
    "                )\n",
    "                result = cur.fetchone()\n",
    "                \n",
    "                if result is None:\n",
    "                    # Company exists, handle based on ignore_company_if_exist flag\n",
    "                    cur.execute(\n",
    "                        \"\"\"\n",
    "                        SELECT \"ID\" FROM companies WHERE name = %s;\n",
    "                        \"\"\",\n",
    "                        (name,)\n",
    "                    )\n",
    "                    company_id = cur.fetchone()[0]\n",
    "                    \n",
    "                    if not ignore_company_if_exist:\n",
    "                        raise ValueError(f\"Company '{name}' already exists.\")\n",
    "                    \n",
    "                    logger.warning(\"Company already exists, ignoring new entry\")\n",
    "\n",
    "                    # Check if additional info needs to be updated\n",
    "                    if additional_info is not None:\n",
    "                        for key, value in additional_info.items():\n",
    "                            # Add column if it doesn't exist\n",
    "                            cur.execute(\n",
    "                                f\"\"\"\n",
    "                                ALTER TABLE companies\n",
    "                                ADD COLUMN IF NOT EXISTS {key} TEXT;\n",
    "                                \"\"\"\n",
    "                            )\n",
    "                            \n",
    "                            # Check current value before updating\n",
    "                            cur.execute(\n",
    "                                f\"\"\"\n",
    "                                SELECT {key} FROM companies WHERE \"ID\" = %s;\n",
    "                                \"\"\",\n",
    "                                (company_id,)\n",
    "                            )\n",
    "                            current_value = cur.fetchone()[0]\n",
    "                            \n",
    "                            # Only update if the value is different\n",
    "                            if current_value != value:\n",
    "                                logger.warning(f\"Overwriting '{key}' for company '{name}' from '{current_value}' to '{value}'.\")\n",
    "                                cur.execute(\n",
    "                                    f\"\"\"\n",
    "                                    UPDATE companies\n",
    "                                    SET {key} = %s\n",
    "                                    WHERE \"ID\" = %s;\n",
    "                                    \"\"\",\n",
    "                                    (value, company_id)\n",
    "                                )\n",
    "                else:\n",
    "                    company_id = result[0]\n",
    "                    \n",
    "                    # Insert additional information for new entry\n",
    "                    if additional_info is not None:\n",
    "                        for key, value in additional_info.to_dict().items():\n",
    "                            # Add column if it doesn't exist\n",
    "                            cur.execute(\n",
    "                                f\"\"\"\n",
    "                                ALTER TABLE companies\n",
    "                                ADD COLUMN IF NOT EXISTS {key} TEXT;\n",
    "                                \"\"\"\n",
    "                            )\n",
    "                            # Update the row with the additional information\n",
    "                            cur.execute(\n",
    "                                f\"\"\"\n",
    "                                UPDATE companies\n",
    "                                SET {key} = %s\n",
    "                                WHERE \"ID\" = %s;\n",
    "                                \"\"\",\n",
    "                                (value, company_id)\n",
    "                            )\n",
    "                    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Got error while writing company name to database: {e}\")\n",
    "        raise e\n",
    "\n",
    "    return company_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Category level\n",
    "#### Main function to write categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def write_categories(categories: dict, company_id: int, category_level_names: list = None) -> t.List[int]:\n",
    "    \n",
    "    \"\"\"\n",
    "    This function writes the categories to the database.\n",
    "\n",
    "    The categories are expected to be in a dictionary with the following structure:\n",
    "    {1:\n",
    "        {\n",
    "            \"category_name\": None\n",
    "        }\n",
    "    2:\n",
    "        {\n",
    "            \"category_name\": [\"parent_category_1\", \"parent_category_2\", ...]\n",
    "        }\n",
    "    ...\n",
    "    }\n",
    "\n",
    "    For the first level, the parent list should be None.\n",
    "\n",
    "    If a category on a lower level has another parent from 2 or more levels above, the cateogry should be\n",
    "    listed under the lowest level parent. (such that the write db function can first write all parents and then the children)\n",
    "    \"\"\"\n",
    "\n",
    "    if category_level_names is not None:\n",
    "        write_category_level_descriptions(category_level_names, company_id)\n",
    "\n",
    "    for i, level in categories.items():\n",
    "        write_category_level(level, company_id)\n",
    "\n",
    "    return True\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions for categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def write_category_level_descriptions(category_level_names, company_id):\n",
    "        \n",
    "    \"\"\"\n",
    "    This function writes the names of the category levels to the database.\n",
    "    \"\"\"\n",
    "\n",
    "    db_credentials = get_db_credentials()[\"con\"]\n",
    "\n",
    "    try:\n",
    "        with psycopg2.connect(db_credentials) as conn:\n",
    "            with conn.cursor() as cur:\n",
    "                for i, name in enumerate(category_level_names):\n",
    "                    cur.execute(\n",
    "                        \"\"\"\n",
    "                        INSERT INTO category_level_descriptions (\"companyID\", level, name)\n",
    "                        VALUES (%s, %s, %s)\n",
    "                        ON CONFLICT (\"companyID\", level) \n",
    "                        DO UPDATE SET name = EXCLUDED.name;\n",
    "                        \"\"\",\n",
    "                        (company_id, i+1, name)\n",
    "                    )\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Got error while writing category level names to database: {e}\")\n",
    "        raise e\n",
    "\n",
    "def write_category_level(categories: list, company_id: int) -> t.Dict[str, int]:\n",
    "    \n",
    "    \"\"\"\n",
    "    This function writes one level of categories into the database and returns a list of the IDs that the database has assigned.\n",
    "    The purpose is to call this function repeatedly for each level of categories.\n",
    "\n",
    "    It will add data to two tables:\n",
    "     - `categories` with the category names and a flag if it is a leaf category\n",
    "     - `category_relations` with the parent-child relationships between categories\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    db_credentials = get_db_credentials()[\"con\"]\n",
    "\n",
    "    try:\n",
    "        with psycopg2.connect(db_credentials) as conn:\n",
    "            with conn.cursor() as cur:\n",
    "                for key, value in categories.items():\n",
    "                    parents = value\n",
    "                    cur.execute(\n",
    "                        \"\"\"\n",
    "                        WITH ins AS (\n",
    "                            INSERT INTO categories (\"companyID\", name)\n",
    "                            VALUES (%s, %s)\n",
    "                            ON CONFLICT (\"companyID\", name) DO NOTHING\n",
    "                            RETURNING \"ID\"\n",
    "                        )\n",
    "                        SELECT \"ID\" FROM ins\n",
    "                        UNION ALL\n",
    "                        SELECT \"ID\" FROM categories\n",
    "                        WHERE \"companyID\" = %s AND name = %s\n",
    "                        LIMIT 1;\n",
    "                        \"\"\",\n",
    "                        (company_id, key, company_id, key)\n",
    "                    )\n",
    "                    category_id = cur.fetchone()[0]\n",
    "                    \n",
    "                    if parents is not None:\n",
    "                        \n",
    "                        for parent in parents:\n",
    "                            cur.execute(\n",
    "                                \"\"\"\n",
    "                                SELECT \"ID\" FROM categories\n",
    "                                WHERE \"companyID\" = %s AND name = %s;\n",
    "                                \"\"\",\n",
    "                                (company_id, parent)\n",
    "                            )\n",
    "                            parent_id = cur.fetchone()[0]\n",
    "                            cur.execute(\n",
    "                                \"\"\"\n",
    "                                INSERT INTO category_relations (\"subID\", \"parentID\")\n",
    "                                VALUES (%s, %s)\n",
    "                                ON CONFLICT (\"subID\", \"parentID\") \n",
    "                                DO NOTHING;\n",
    "                                \"\"\",\n",
    "                                (category_id, parent_id)\n",
    "                            )\n",
    "        return\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Got error while writing category level names to database: {e}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Product data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#| export\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrite_products\u001b[39m(products: \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mDataFrame, company_id: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m      5\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m    This function writes the products to the database.\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     db_credentials \u001b[38;5;241m=\u001b[39m get_db_credentials()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcon\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "\n",
    "def write_products(products: pd.DataFrame, company_id: int) -> None:\n",
    "    \n",
    "    \"\"\"\n",
    "    This function writes the products to the database.\n",
    "\n",
    "    The input must be a dataframe with the following structure:\n",
    "    First column: product name (column name is irrelevant)\n",
    "    Second column: category name (column name is irrelevant)\n",
    "\n",
    "    Note that each product may have more than one category. \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    db_credentials = get_db_credentials()[\"con\"]\n",
    "\n",
    "    try:\n",
    "        with psycopg2.connect(db_credentials) as conn:\n",
    "            with conn.cursor() as cur:\n",
    "                cur.execute(\n",
    "                    \"\"\" \n",
    "                    SELECT products.name\n",
    "                    FROM companies \n",
    "                    JOIN categories ON companies.\"ID\" = categories.\"companyID\"\n",
    "                    JOIN product_categories ON product_categories.\"categoryID\" = categories.\"ID\"\n",
    "                    JOIN products ON product_categories.\"productID\" = products.\"ID\"\n",
    "                    ;\n",
    "                    \"\"\",\n",
    "                    (company_id,)\n",
    "                )\n",
    "\n",
    "                names = cur.fetchall()\n",
    "                names = [name[0] for name in names]\n",
    "        \n",
    "                products_filtered = products[~products.iloc[:,0].isin(names)]\n",
    "\n",
    "                products_filtered_list = products_filtered.iloc[:,0].tolist()\n",
    "                products_filtered_list = list(set(products_filtered_list))\n",
    "                \n",
    "                inserted_ids = []\n",
    "                for product in products_filtered_list:\n",
    "                    cur.execute(\n",
    "                        \"\"\"\n",
    "                        INSERT INTO products (name)\n",
    "                        VALUES (%s)\n",
    "                        RETURNING \"ID\";\n",
    "                        \"\"\",\n",
    "                        (product,)\n",
    "                    )\n",
    "                    inserted_id = cur.fetchone()[0]  # Fetch the generated ID\n",
    "                    inserted_ids.append(inserted_id)\n",
    "\n",
    "                cur.execute(\n",
    "                    \"\"\" \n",
    "                    SELECT categories.\"ID\", categories.name\n",
    "                    FROM companies \n",
    "                    JOIN categories ON companies.\"ID\" = categories.\"companyID\"\n",
    "                    WHERE companies.\"ID\" = %s;\n",
    "                    \"\"\",\n",
    "                    (company_id,)\n",
    "                )\n",
    "\n",
    "                category_names = cur.fetchall()\n",
    "\n",
    "                category_names_df = pd.DataFrame(category_names, columns=[\"ID\", \"name\"])\n",
    "\n",
    "                products_filtered = products_filtered.merge(category_names_df, left_on=products_filtered.columns[1], right_on=\"name\", how=\"left\")\n",
    "                products_filtered[\"product_id\"] = inserted_ids\n",
    "\n",
    "                products_filtered = products_filtered[[\"product_id\", \"ID\"]]\n",
    "                products_filtered[\"product_id\"] = products_filtered[\"product_id\"].astype(int)\n",
    "                products_filtered[\"ID\"] = products_filtered[\"ID\"].astype(int)\n",
    "\n",
    "                values_to_insert = [tuple(row) for row in products_filtered.itertuples(index=False)]\n",
    "\n",
    "                cur.executemany(\n",
    "                    \"\"\"\n",
    "                    INSERT INTO product_categories (\"productID\", \"categoryID\")\n",
    "                    VALUES (%s, %s);\n",
    "                    \"\"\",\n",
    "                    values_to_insert  # Use the converted list of tuples\n",
    "                )\n",
    "                \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Got error while writing products to database: {e}\")\n",
    "        raise e "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store level\n",
    "#### Main function to write stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def write_stores(store_regions: pd.DataFrame, company_id) -> None:\n",
    "    \"\"\"\n",
    "    This function writes the store data to the database.\n",
    "    \"\"\"\n",
    "\n",
    "    db_credentials = get_db_credentials()[\"con\"]\n",
    "\n",
    "\n",
    "    try:\n",
    "        with psycopg2.connect(db_credentials) as conn:\n",
    "            with conn.cursor() as cur:\n",
    "\n",
    "                store_regions = add_region_ids(store_regions, cur)\n",
    "\n",
    "                cur.executemany(\n",
    "                    \n",
    "                    \"\"\"\n",
    "                    INSERT INTO stores (\"name\", \"regionID\", \"companyID\")\n",
    "                    VALUES (%s, %s, %s)\n",
    "                    ON CONFLICT (\"name\", \"companyID\") DO NOTHING;\n",
    "                    \"\"\",\n",
    "                    (store_regions[[\"name\", \"region_id\"]].assign(companyID=company_id).values.tolist())\n",
    "                )\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Got error while writing stores to database: {e}\")\n",
    "        raise e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions for write_stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_region_ids(cur, country, abbreviation, type_):\n",
    "\n",
    "    cur.execute(\n",
    "        \"\"\"\n",
    "        WITH RECURSIVE RegionHierarchy AS (\n",
    "            -- Base case: Find the ID of the specified country region\n",
    "            SELECT \"ID\", \"abbreviation\", \"type\"\n",
    "            FROM regions\n",
    "            WHERE \"abbreviation\" = %s AND \"type\" = %s\n",
    "\n",
    "            UNION ALL\n",
    "\n",
    "            -- Recursive case: Find all regions whose parent_regionID matches the ID of the regions in the previous level\n",
    "            SELECT r.\"ID\", r.\"abbreviation\", r.\"type\"\n",
    "            FROM regions r\n",
    "            INNER JOIN RegionHierarchy rh ON r.\"parent_regionID\" = rh.\"ID\"\n",
    "        )\n",
    "        -- Select the desired region with the specified abbreviation and type\n",
    "        SELECT \"ID\"\n",
    "        FROM RegionHierarchy\n",
    "        WHERE \"abbreviation\" = %s AND \"type\" = %s;\n",
    "        \"\"\",\n",
    "        (country, 'country', abbreviation, type_)\n",
    "    )\n",
    "    region_id = cur.fetchone()\n",
    "\n",
    "    return region_id\n",
    "\n",
    "def add_region_ids(data, cur):\n",
    "    \"\"\"\n",
    "    Adds region IDs to the given DataFrame by mapping region, type, and country.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): Input DataFrame containing `region`, `type`, and `country` columns.\n",
    "        cur (psycopg2.cursor): Database cursor for querying region IDs.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with an additional `region_id` column.\n",
    "    \"\"\"\n",
    "    # Extract unique region, type, and country combinations\n",
    "    region_identifier = data[[\"region\", \"type\", \"country\"]].drop_duplicates()\n",
    "\n",
    "    # Create a mapping for region IDs\n",
    "    region_id_mapping = []\n",
    "    for abbreviation, type_, country in region_identifier.itertuples(index=False):\n",
    "        region_id = get_region_ids(cur, country, abbreviation, type_)\n",
    "\n",
    "        if region_id:\n",
    "            region_id = int(region_id[0])\n",
    "            region_id_mapping.append((abbreviation, type_, country, region_id))\n",
    "        else:\n",
    "            # Raise an error if the region ID is not found\n",
    "            raise ValueError(\n",
    "                f\"No matching region found for abbreviation={abbreviation}, type={type_}, country={country}.\"\n",
    "            )\n",
    "\n",
    "    # Convert mapping to a DataFrame\n",
    "    region_id_mapping_df = pd.DataFrame(\n",
    "        region_id_mapping,\n",
    "        columns=[\"region\", \"type\", \"country\", \"region_id\"]\n",
    "    )\n",
    "\n",
    "    # Merge the region ID mapping back into the original data\n",
    "    data = data.merge(region_id_mapping_df, on=[\"region\", \"type\", \"country\"], how=\"left\")\n",
    "\n",
    "    # Check for any unmatched rows (this should not happen due to the error raised earlier)\n",
    "    if data[\"region_id\"].isnull().any():\n",
    "        raise ValueError(\"Some rows could not be assigned a region_id.\")\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SKU writing\n",
    "#### Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def write_skus(store_item_combinations: pd.DataFrame, company_id: int) -> None:\n",
    "    \"\"\"\n",
    "    Writes SKU data to the database.\n",
    "\n",
    "    For each store-item combination:\n",
    "    - Fetches the product ID by matching `item_name` in the `products` table, filtering by company.\n",
    "    - Fetches the store ID by matching `store_name` in the `stores` table, filtering by company.\n",
    "    - Inserts the resulting combinations into the `sku_table`.\n",
    "\n",
    "    Args:\n",
    "        store_item_combinations (pd.DataFrame): DataFrame with columns `store_name` and `item_name`.\n",
    "        company_id (int): ID of the company for filtering relevant data.\n",
    "\n",
    "    Raises:\n",
    "        Exception: If any errors occur during the database operation.\n",
    "    \"\"\"\n",
    "    db_credentials = get_db_credentials()[\"con\"]\n",
    "\n",
    "    try:\n",
    "        with psycopg2.connect(db_credentials) as conn:\n",
    "            with conn.cursor() as cur:\n",
    "                # Fetch product IDs\n",
    "                product_mapping = get_product_ids(\n",
    "                    cur=cur, \n",
    "                    company_id=company_id, \n",
    "                    item_name_list=store_item_combinations[\"item_name\"].unique().tolist()\n",
    "                )\n",
    "\n",
    "                # Fetch store IDs\n",
    "                store_mapping = get_store_ids(\n",
    "                    cur=cur, \n",
    "                    company_id=company_id, \n",
    "                    store_name_list=store_item_combinations[\"store_name\"].unique().tolist()\n",
    "                )\n",
    "\n",
    "                # Merge product and store IDs with the input DataFrame\n",
    "                merged_data = store_item_combinations.merge(\n",
    "                    product_mapping, on=\"item_name\", how=\"left\"\n",
    "                ).merge(\n",
    "                    store_mapping, on=\"store_name\", how=\"left\"\n",
    "                )\n",
    "\n",
    "                # Check for unmatched rows\n",
    "                if merged_data[\"productID\"].isnull().any():\n",
    "                    unmatched_items = merged_data.loc[merged_data[\"productID\"].isnull(), \"item_name\"].unique()\n",
    "                    raise ValueError(f\"Unmatched item_names: {unmatched_items}\")\n",
    "                if merged_data[\"storeID\"].isnull().any():\n",
    "                    unmatched_stores = merged_data.loc[merged_data[\"storeID\"].isnull(), \"store_name\"].unique()\n",
    "                    raise ValueError(f\"Unmatched store_names: {unmatched_stores}\")\n",
    "\n",
    "                # Prepare data for insertion\n",
    "                sku_data = merged_data[[\"productID\", \"storeID\"]].drop_duplicates().values.tolist()\n",
    "\n",
    "                # Insert data into the SKU table\n",
    "                cur.executemany(\n",
    "                    \"\"\"\n",
    "                    INSERT INTO sku_table (\"productID\", \"storeID\")\n",
    "                    VALUES (%s, %s)\n",
    "                    ON CONFLICT (\"productID\", \"storeID\") DO NOTHING;\n",
    "                    \"\"\",\n",
    "                    sku_data\n",
    "                )\n",
    "                conn.commit()\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error while writing SKUs to the database: {e}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions for SKU writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_product_ids(cur, company_id, item_name_list):\n",
    "    \"\"\"\n",
    "    Fetch product IDs for a given company and a list of item names.\n",
    "\n",
    "    Args:\n",
    "        cur (psycopg2.cursor): Database cursor for executing SQL commands.\n",
    "        company_id (int): ID of the company.\n",
    "        item_name_list (list): List of item names.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame mapping product IDs to item names.\n",
    "    \"\"\"\n",
    "    cur.execute(\n",
    "        \"\"\"\n",
    "        SELECT products.\"ID\" AS productID, products.name AS item_name\n",
    "        FROM products\n",
    "        JOIN product_categories ON products.\"ID\" = product_categories.\"productID\"\n",
    "        JOIN categories ON product_categories.\"categoryID\" = categories.\"ID\"\n",
    "        WHERE categories.\"companyID\" = %s AND products.name = ANY(%s);\n",
    "        \"\"\",\n",
    "        (company_id, item_name_list)\n",
    "    )\n",
    "    product_mapping = cur.fetchall()\n",
    "    return pd.DataFrame(product_mapping, columns=[\"productID\", \"item_name\"])\n",
    "\n",
    "def get_store_ids(cur, company_id, store_name_list):\n",
    "    \"\"\"\n",
    "    Fetch store IDs for a given company and a list of store names.\n",
    "\n",
    "    Args:\n",
    "        cur (psycopg2.cursor): Database cursor for executing SQL commands.\n",
    "        company_id (int): ID of the company.\n",
    "        store_name_list (list): List of store names.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame mapping store IDs to store names.\n",
    "    \"\"\"\n",
    "    cur.execute(\n",
    "        \"\"\"\n",
    "        SELECT \"ID\", name \n",
    "        FROM stores \n",
    "        WHERE \"companyID\" = %s AND name = ANY(%s);\n",
    "        \"\"\",\n",
    "        (company_id, list(store_name_list))\n",
    "    )\n",
    "    store_id_mapping = cur.fetchall()\n",
    "    return pd.DataFrame(store_id_mapping, columns=[\"storeID\", \"store_name\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datapoint-level data\n",
    "#### Datapoint IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def write_datapoints(sales: pd.DataFrame, company_id: int) -> None:\n",
    "    \"\"\"\n",
    "    Writes datapoints to the `datapoints` table in the database.\n",
    "\n",
    "    The datapoints consist of `skuID` and `dateID`, resolved based on `store_name`, `item_name`, and `date`.\n",
    "\n",
    "    Args:\n",
    "        sales (pd.DataFrame): DataFrame containing `store_name`, `item_name`, `date`, and additional data.\n",
    "        company_id (int): ID of the company for filtering relevant data.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If any `store_name`, `item_name`, or `date` cannot be matched or if duplicate rows are found.\n",
    "    \"\"\"\n",
    "    db_credentials = get_db_credentials()[\"con\"]\n",
    "\n",
    "    try:\n",
    "        with psycopg2.connect(db_credentials) as conn:\n",
    "            with conn.cursor() as cur:\n",
    "                # Step 1: Resolve SKU IDs\n",
    "                store_product_names = sales[[\"store_name\", \"item_name\"]].drop_duplicates()\n",
    "                sku_ids = get_sku_ids(cur, store_product_names, company_id)\n",
    "\n",
    "                # Merge SKU IDs into the sales DataFrame based on `store_name` and `item_name`\n",
    "                sales = sales.merge(sku_ids, on=[\"store_name\", \"item_name\"], how=\"left\")\n",
    "\n",
    "                # Step 2: Resolve Date IDs\n",
    "                unique_dates = sales[\"date\"].drop_duplicates().tolist()\n",
    "                date_ids = get_date_ids(cur, unique_dates)\n",
    "                date_ids[\"date\"] = pd.to_datetime(date_ids[\"date\"], errors=\"coerce\")\n",
    "\n",
    "                # Merge Date IDs into the sales DataFrame\n",
    "                sales[\"date\"] = pd.to_datetime(sales[\"date\"], errors=\"coerce\")\n",
    "                sales = sales.merge(date_ids, on=\"date\", how=\"left\")\n",
    "\n",
    "                # Check for unmatched rows\n",
    "                if sales[\"skuID\"].isnull().any():\n",
    "                    unmatched_skus = sales.loc[sales[\"skuID\"].isnull(), [\"store_name\", \"item_name\"]].drop_duplicates()\n",
    "                    raise ValueError(f\"Unmatched SKUs: {unmatched_skus.to_dict(orient='records')}\")\n",
    "                if sales[\"dateID\"].isnull().any():\n",
    "                    unmatched_dates = sales.loc[sales[\"dateID\"].isnull(), \"date\"].unique()\n",
    "                    raise ValueError(f\"Unmatched dates: {unmatched_dates}\")\n",
    "\n",
    "                # Check for duplicate rows in `skuID` and `dateID`\n",
    "                if sales[[\"skuID\", \"dateID\"]].duplicated().any():\n",
    "                    duplicate_rows = sales[sales[[\"skuID\", \"dateID\"]].duplicated(keep=False)]\n",
    "                    raise ValueError(f\"Duplicate rows found in the data: {duplicate_rows}\")\n",
    "\n",
    "                datapoints_data = sales[[\"skuID\", \"dateID\"]]\n",
    "\n",
    "                # Insert data into the `datapoints` table in batches\n",
    "                datapoint_ids = insert_multi_rows(\n",
    "                    data_to_insert=datapoints_data,\n",
    "                    table_name=\"datapoints\",\n",
    "                    column_names=[\"skuID\", \"dateID\"],\n",
    "                    types=[int, int],\n",
    "                    cur=cur,\n",
    "                    conn=conn,\n",
    "                    return_with_ids=True,\n",
    "                    unique_columns=[\"skuID\", \"dateID\"]\n",
    "                )\n",
    "        return datapoint_ids\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error while writing datapoints to the database: {e}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time-sku specific data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def write_sales(sales: pd.DataFrame, company_id, datapoint_ids) -> None:\n",
    "\n",
    "    \"\"\"\n",
    "    This function writes the sales data to the database.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    write_SKU_date_specific_data(\n",
    "        data=sales,\n",
    "        datapoint_ids = datapoint_ids,\n",
    "        variable_name=\"sales\",\n",
    "        variable_type=float,\n",
    "        table_name=\"sales\",\n",
    "        company_id=company_id,\n",
    "    )\n",
    "\n",
    "def write_prices(prices: pd.DataFrame, company_id, datapoint_ids) -> None:\n",
    "\n",
    "    \"\"\"\n",
    "    This function writes the prices data to the database.\n",
    "    \"\"\"\n",
    "\n",
    "    write_SKU_date_specific_data(\n",
    "        data=prices,\n",
    "        datapoint_ids = datapoint_ids,\n",
    "        variable_name=\"price\",\n",
    "        variable_type=float,\n",
    "        table_name=\"prices\",\n",
    "        company_id=company_id,\n",
    "    )\n",
    "\n",
    "def write_sold_flag(sold_flags: pd.DataFrame, company_id, datapoint_ids) -> None:\n",
    "\n",
    "    \"\"\"\n",
    "    This function writes the sold flag data to the database.\n",
    "    \"\"\"\n",
    "\n",
    "    write_SKU_date_specific_data(\n",
    "        data=sold_flags,\n",
    "        datapoint_ids = datapoint_ids,\n",
    "        variable_name=\"name\",\n",
    "        variable_type=str,\n",
    "        table_name=\"flags\",\n",
    "        company_id=company_id,\n",
    "        name_in_df=\"sold_flag\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def write_SKU_date_specific_data(\n",
    "    data: pd.DataFrame,\n",
    "    datapoint_ids,\n",
    "    variable_name: str,\n",
    "    variable_type: callable,\n",
    "    table_name: str,\n",
    "    company_id: int,\n",
    "    name_in_df=None\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Writes SKU and date-specific data to the database using the new `datapointID` schema.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): Input data containing `item_name`, `store_name`, `date`, and the variable to insert.\n",
    "        variable_name (str): The name of the variable/column to be inserted into the database.\n",
    "        variable_type (callable): The type to cast the variable's values (e.g., int, float, str).\n",
    "        table_name (str): The name of the database table to insert into.\n",
    "        company_id (int): The company ID for filtering relevant records.\n",
    "        name_in_df (str, optional): Column name in the DataFrame for the variable. Defaults to `variable_name`.\n",
    "    \"\"\"\n",
    "    db_credentials = get_db_credentials()[\"con\"]\n",
    "\n",
    "    try:\n",
    "        with psycopg2.connect(db_credentials) as conn:\n",
    "            with conn.cursor() as cur:\n",
    "                # Fetch `skuID` mappings\n",
    "                logger.info(\"-- in write SKU date specific data -- getting sku IDs\")\n",
    "                store_product_names = data[[\"store_name\", \"item_name\"]].drop_duplicates()\n",
    "                sku_mapping = get_sku_ids(cur, store_product_names, company_id)\n",
    "\n",
    "                # Fetch `dateID` mappings\n",
    "                logger.info(\"-- in write SKU date specific data -- getting date IDs\")\n",
    "                unique_dates = data[\"date\"].drop_duplicates()\n",
    "                date_mapping = get_date_ids(cur, unique_dates)\n",
    "                date_mapping[\"date\"] = pd.to_datetime(date_mapping[\"date\"], errors=\"coerce\")\n",
    "                \n",
    "                # Merge `skuID` and `dateID` into the input data\n",
    "                logger.info(\"-- in write SKU date specific data -- merging sku IDs and date IDs\")\n",
    "                data = data.merge(sku_mapping, on=[\"store_name\", \"item_name\"], how=\"left\")\n",
    "                data[\"date\"] = pd.to_datetime(data[\"date\"], errors=\"coerce\")\n",
    "                data = data.merge(date_mapping, on=\"date\", how=\"left\")\n",
    "                data.drop(columns=[\"store_name\", \"item_name\", \"date\"], inplace=True)\n",
    "\n",
    "                # show ram usage of data\n",
    "                logger.info(f\"-- in write SKU date specific data -- Memory usage of data: {data.memory_usage().sum() / 1024 / 1024 ** 2:.2f} GB\")\n",
    "\n",
    "                # Check for unmatched mappings\n",
    "                if data[\"skuID\"].isnull().any():\n",
    "                    unmatched_skus = data.loc[data[\"skuID\"].isnull(), [\"store_name\", \"item_name\"]].drop_duplicates()\n",
    "                    raise ValueError(f\"Unmatched SKUs: {unmatched_skus.to_dict(orient='records')}\")\n",
    "                if data[\"dateID\"].isnull().any():\n",
    "                    unmatched_dates = data.loc[data[\"dateID\"].isnull(), \"date\"].unique()\n",
    "                    raise ValueError(f\"Unmatched dates: {unmatched_dates}\")\n",
    "\n",
    "                logger.info(\"-- in write SKU date specific data -- getting checking for duplicates\")\n",
    "                # Fetch `datapointID` for `skuID` and `dateID` combinations\n",
    "                datapoint_combinations = data[[\"skuID\", \"dateID\"]]\n",
    "\n",
    "                # Check for duplicate combinations of `skuID` and `dateID`\n",
    "                if datapoint_combinations.duplicated().any():\n",
    "                    duplicate_rows = datapoint_combinations[datapoint_combinations.duplicated(keep=False)]\n",
    "                    raise ValueError(f\"Duplicate rows found in the data: {duplicate_rows}\")\n",
    "                data.drop(columns=[\"storeID\", \"productID\"], inplace=True)\n",
    "\n",
    "                # Merge `datapointID` into the input data\n",
    "                # rename column ID to datapointID in the datapoint_IDs\n",
    "                logger.info(\"-- in write SKU date specific data -- merging datapoint IDs\")\n",
    "                datapoint_ids = datapoint_ids.rename(columns={\"ID\": \"datapointID\"})\n",
    "                data = data.merge(datapoint_ids, on=[\"skuID\", \"dateID\"], how=\"left\")\n",
    "\n",
    "                # Check for unmatched `datapointID`\n",
    "                logger.info(\"-- in write SKU date specific data -- checking for unmatched datapoints\")\n",
    "                # if data[\"datapointID\"].isnull().any():\n",
    "                #     # unmatched_datapoints = data.loc[data[\"datapointID\"].isnull(), [\"skuID\", \"dateID\"]].drop_duplicates()\n",
    "                #     # raise ValueError(f\"Unmatched datapoints: {unmatched_datapoints.to_dict(orient='records')}\")\n",
    "                #     raise ValueError(f\"Unmatched datapoints\")\n",
    "                data.drop(columns=[\"skuID\", \"dateID\"], inplace=True)\n",
    "\n",
    "                # Prepare data for insertion\n",
    "                logger.info(\"-- in write SKU date specific data -- preparing data for insertion\")\n",
    "                if name_in_df is None:\n",
    "                    name_in_df = variable_name\n",
    "                data_to_write = data[[\"datapointID\", name_in_df]].copy()\n",
    "                data_to_write[name_in_df] = data_to_write[name_in_df].astype(variable_type)\n",
    "\n",
    "                # Insert data into the specified table\n",
    "                logger.info(\"-- in write SKU date specific data -- inserting data\")\n",
    "                insert_multi_rows(\n",
    "                    data_to_insert=data_to_write,\n",
    "                    table_name=table_name,\n",
    "                    column_names=[\"datapointID\", variable_name],\n",
    "                    types=[int, variable_type],\n",
    "                    cur=cur,\n",
    "                    conn=conn\n",
    "                )\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error while writing {variable_name} data to the database: {e}\")\n",
    "        raise e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions for time-sku specific data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_sku_ids(cur, store_product_names: pd.DataFrame, company_id: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetch skuIDs for given combinations of `store_name` and `item_name`.\n",
    "\n",
    "    Args:\n",
    "        cur (psycopg2.cursor): Database cursor for executing SQL commands.\n",
    "        store_product_names (pd.DataFrame): DataFrame with two columns: `store_name` and `item_name`.\n",
    "        company_id (int): ID of the company for filtering relevant data.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame mapping `skuID`, `storeID`, `productID`, `store_name`, and `item_name`.\n",
    "    \"\"\"\n",
    "    # Step 1: Resolve store IDs\n",
    "    store_ids = get_store_ids(\n",
    "        cur=cur,\n",
    "        company_id=company_id,\n",
    "        store_name_list=store_product_names[\"store_name\"].unique().tolist()\n",
    "    )\n",
    "\n",
    "    # Step 2: Resolve product IDs\n",
    "    product_ids = get_product_ids(\n",
    "        cur=cur,\n",
    "        company_id=company_id,\n",
    "        item_name_list=store_product_names[\"item_name\"].unique().tolist()\n",
    "    )\n",
    "\n",
    "    # Step 3: Merge store and product IDs with input DataFrame\n",
    "    store_product_ids = store_product_names.merge(store_ids, on=\"store_name\", how=\"left\")\n",
    "    store_product_ids = store_product_ids.merge(product_ids, on=\"item_name\", how=\"left\")\n",
    "\n",
    "    # Check for unmatched rows\n",
    "    if store_product_ids[\"storeID\"].isnull().any():\n",
    "        unmatched_stores = store_product_ids.loc[store_product_ids[\"storeID\"].isnull(), \"store_name\"].unique()\n",
    "        raise ValueError(f\"Unmatched store names: {unmatched_stores}\")\n",
    "    if store_product_ids[\"productID\"].isnull().any():\n",
    "        unmatched_products = store_product_ids.loc[store_product_ids[\"productID\"].isnull(), \"item_name\"].unique()\n",
    "        raise ValueError(f\"Unmatched product names: {unmatched_products}\")\n",
    "\n",
    "    # Step 4: Use a temporary table for efficient querying\n",
    "    temp_table_name = \"temp_store_product\"\n",
    "    temp_data = store_product_ids[[\"storeID\", \"productID\"]]\n",
    "\n",
    "    # Create temporary table\n",
    "    cur.execute(f\"\"\"\n",
    "        CREATE TEMP TABLE {temp_table_name} (\n",
    "            storeID INT,\n",
    "            productID INT\n",
    "        ) ON COMMIT DROP;\n",
    "    \"\"\")\n",
    "\n",
    "    # Insert data into the temporary table\n",
    "    psycopg2.extras.execute_batch(\n",
    "        cur,\n",
    "        f\"\"\"\n",
    "        INSERT INTO {temp_table_name} (storeID, productID)\n",
    "        VALUES (%s, %s);\n",
    "        \"\"\",\n",
    "        temp_data.values.tolist()\n",
    "    )\n",
    "\n",
    "    # Query sku_table using a JOIN\n",
    "    cur.execute(f\"\"\"\n",
    "        SELECT sku_table.\"ID\" AS skuID, sku_table.\"storeID\", sku_table.\"productID\"\n",
    "        FROM sku_table\n",
    "        INNER JOIN {temp_table_name}\n",
    "        ON sku_table.\"storeID\" = {temp_table_name}.storeID\n",
    "        AND sku_table.\"productID\" = {temp_table_name}.productID;\n",
    "    \"\"\")\n",
    "\n",
    "    # Fetch and return results\n",
    "    result = cur.fetchall()\n",
    "    sku_df = pd.DataFrame(result, columns=[\"skuID\", \"storeID\", \"productID\"])\n",
    "\n",
    "    # Merge the original store_name and item_name back into the results\n",
    "    final_result = sku_df.merge(store_product_ids, on=[\"storeID\", \"productID\"], how=\"left\")\n",
    "    \n",
    "    return final_result[[\"skuID\", \"storeID\", \"productID\", \"store_name\", \"item_name\"]]\n",
    "\n",
    "\n",
    "def get_datapoint_ids(cur, datapoint_combinations: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetch `datapointID` for given combinations of `skuID` and `dateID`.\n",
    "\n",
    "    Args:\n",
    "        cur (psycopg2.cursor): Database cursor for executing SQL commands.\n",
    "        datapoint_combinations (pd.DataFrame): DataFrame with two columns: `skuID` and `dateID`.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame mapping `datapointID`, `skuID`, and `dateID`.\n",
    "    \"\"\"\n",
    "    # Check for valid columns\n",
    "    if not {\"skuID\", \"dateID\"}.issubset(datapoint_combinations.columns):\n",
    "        raise ValueError(\"Input DataFrame must contain 'skuID' and 'dateID' columns.\")\n",
    "\n",
    "    # Convert combinations to a list of tuples for use in the query\n",
    "    combinations_list = datapoint_combinations[[\"skuID\", \"dateID\"]].drop_duplicates().values.tolist()\n",
    "\n",
    "    try:\n",
    "        # Create a temporary table to store the combinations\n",
    "        cur.execute(\"\"\"\n",
    "            CREATE TEMP TABLE temp_datapoints (\n",
    "                \"skuID\" INTEGER,\n",
    "                \"dateID\" INTEGER\n",
    "            ) ON COMMIT DROP;\n",
    "        \"\"\")\n",
    "\n",
    "        logger.info(\"Adding into table for temp_datapoints\")\n",
    "        # Insert the combinations into the temporary table\n",
    "        psycopg2.extras.execute_values(\n",
    "            cur,\n",
    "            \"\"\"\n",
    "            INSERT INTO temp_datapoints (\"skuID\", \"dateID\")\n",
    "            VALUES %s;\n",
    "            \"\"\",\n",
    "            combinations_list\n",
    "        )\n",
    "\n",
    "        # Query for datapointIDs\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT d.\"ID\", d.\"skuID\", d.\"dateID\"\n",
    "            FROM datapoints d\n",
    "            INNER JOIN temp_datapoints t\n",
    "            ON d.\"skuID\" = t.\"skuID\" AND d.\"dateID\" = t.\"dateID\";\n",
    "        \"\"\")\n",
    "\n",
    "        # Fetch results and return as a DataFrame\n",
    "        result = cur.fetchall()\n",
    "        return pd.DataFrame(result, columns=[\"datapointID\", \"skuID\", \"dateID\"])\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error while fetching datapoint IDs: {e}\")\n",
    "        raise e\n",
    "\n",
    "\n",
    "def get_date_ids(cur, dates_list):\n",
    "    \"\"\"\n",
    "    Fetch date IDs for a given list of dates.\n",
    "\n",
    "    Args:\n",
    "        cur (psycopg2.cursor): Database cursor for executing SQL commands.\n",
    "        dates_list (list): List of dates.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame mapping date IDs to dates.\n",
    "    \"\"\"\n",
    "    cur.execute(\n",
    "        \"\"\"\n",
    "        SELECT \"ID\", date \n",
    "        FROM dates \n",
    "        WHERE date = ANY(%s::date[]);\n",
    "        \"\"\",\n",
    "        (list(dates_list),)\n",
    "    )\n",
    "    date_id_mapping = cur.fetchall()\n",
    "    return pd.DataFrame(date_id_mapping, columns=[\"dateID\", \"date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time-region specific data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def write_time_region_features(\n",
    "    time_region_features: pd.DataFrame,\n",
    "    name_description: [str, str], # containing name and description of the feature\n",
    "    company_id: int\n",
    "\n",
    "):\n",
    "    \"\"\"\n",
    "\n",
    "    This function writes data into the database whose values are specific to a \n",
    "    time-stamps and regions\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    db_credentials = get_db_credentials()[\"con\"]\n",
    "\n",
    "    try:\n",
    "        with psycopg2.connect(db_credentials) as conn:\n",
    "            with conn.cursor() as cur:\n",
    "\n",
    "\n",
    "                # add name and description to the time_region_features_description table\n",
    "                cur.execute(\n",
    "                    \"\"\"\n",
    "                    WITH inserted AS (\n",
    "                        INSERT INTO time_region_features_description (\"name\", \"description\")\n",
    "                        VALUES (%s, %s)\n",
    "                        ON CONFLICT (\"name\") DO NOTHING\n",
    "                        RETURNING \"ID\"\n",
    "                    )\n",
    "                    SELECT \"ID\" FROM inserted\n",
    "                    UNION ALL\n",
    "                    SELECT \"ID\" FROM time_region_features_description WHERE \"name\" = %s;\n",
    "                    \"\"\",\n",
    "                    (name_description[0], name_description[1], name_description[0])\n",
    "                )\n",
    "\n",
    "                feature_id = cur.fetchone()[0]\n",
    "\n",
    "                time_region_features[\"trfID\"] = feature_id\n",
    "\n",
    "                # add link to company to the time_region_features_description table\n",
    "                cur.execute(\n",
    "                    \"\"\"\n",
    "                    INSERT INTO time_region_features_company (\"companyID\", \"trfID\")\n",
    "                    VALUES (%s, %s)\n",
    "                    ON CONFLICT DO NOTHING;\n",
    "                    \"\"\",\n",
    "                    (company_id, feature_id)\n",
    "                )\n",
    "                \n",
    "                # add features to the time_region_features table\n",
    "                time_region_features = add_region_ids(time_region_features, cur)\n",
    "\n",
    "                # add date features\n",
    "                time_region_features[\"date\"] = pd.to_datetime(time_region_features[\"date\"], errors=\"coerce\")\n",
    "                date_ids = get_date_ids(cur, time_region_features[\"date\"].unique())\n",
    "                date_ids[\"date\"] = pd.to_datetime(date_ids[\"date\"], errors=\"coerce\")\n",
    "                time_region_features = time_region_features.merge(date_ids, on=\"date\", how=\"left\")\n",
    "\n",
    "                cur.executemany(\n",
    "\n",
    "                    \"\"\"\n",
    "                    INSERT INTO time_region_features (\"dateID\", \"regionID\", \"trfID\", \"value\")\n",
    "                    VALUES (%s, %s, %s, %s)\n",
    "                    ON CONFLICT (\"dateID\", \"regionID\", \"trfID\") DO NOTHING;\n",
    "                    \"\"\",\n",
    "\n",
    "                    (time_region_features[[\"dateID\", \"region_id\", \"trfID\", \"feature_value\"]].values.tolist())\n",
    "                )\n",
    "\n",
    "                conn.commit()\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Got error while writing time region features to database: {e}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
