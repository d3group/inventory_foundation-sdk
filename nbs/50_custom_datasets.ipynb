{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom datasets\n",
    "\n",
    "> Additional datasets for kedro pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp custom_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from kedro.io import AbstractDataset\n",
    "\n",
    "from kedro.config import OmegaConfigLoader\n",
    "from pathlib import Path\n",
    "\n",
    "from kedro.framework.project import settings\n",
    "\n",
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "\n",
    "import typing as t\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General function for simple input/output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class AddRowDataset(AbstractDataset):\n",
    "    \n",
    "    \"\"\"\n",
    "    Adds or update one row to a SQL table, if it does not exist.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        table: str,\n",
    "        column_names: t.List,\n",
    "        credentials: str,\n",
    "        unique_columns: t.List,\n",
    "        load_args = None,\n",
    "        save_args = None\n",
    "    ):\n",
    "\n",
    "        self.unique_columns = unique_columns\n",
    "        self.table = table\n",
    "        self.column_names = column_names\n",
    "        self.db_credentials = credentials\n",
    "        self.save_args = save_args or {}\n",
    "        self.load_args = load_args or {}\n",
    "    \n",
    "    def _describe(self) -> t.Dict[str, t.Any]:\n",
    "        \"\"\"Returns a dict that describes the attributes of the dataset.\"\"\"\n",
    "        return dict(\n",
    "            table=self.table,\n",
    "            column_names=self.column_names,\n",
    "        )\n",
    "\n",
    "    def _load(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get all entries from all specified columns in the table.\n",
    "        \"\"\"\n",
    "\n",
    "        return_all_columns = self.load_args.get(\"return_all_columns\", False)\n",
    "\n",
    "        try:\n",
    "            with psycopg2.connect(self.db_credentials['con']) as conn:\n",
    "                with conn.cursor() as cursor:\n",
    "                    \n",
    "                    if return_all_columns:\n",
    "\n",
    "                        # Fetch all rows\n",
    "                        cursor.execute(\n",
    "                            f\"SELECT * FROM {self.table}\"\n",
    "                        )\n",
    "                        data = cursor.fetchall()\n",
    "\n",
    "                        # Fetch column names in the correct order from the database\n",
    "                        cursor.execute(\n",
    "                            \"\"\"\n",
    "                            SELECT a.attname\n",
    "                            FROM pg_attribute a\n",
    "                            JOIN pg_class c ON a.attrelid = c.oid\n",
    "                            JOIN pg_namespace n ON c.relnamespace = n.oid\n",
    "                            WHERE c.relname = %s AND a.attnum > 0 AND NOT a.attisdropped\n",
    "                            ORDER BY a.attnum\n",
    "                            \"\"\",\n",
    "                            (self.table,)\n",
    "                        )\n",
    "                        columns = [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "                    else:\n",
    "                        # Use the specified column names\n",
    "                        columns = self.column_names\n",
    "                        cursor.execute(\n",
    "                            f\"SELECT {','.join(self.column_names)} FROM {self.table}\"\n",
    "                        )\n",
    "                        data = cursor.fetchall()\n",
    "\n",
    "            # Create a DataFrame with the data and columns\n",
    "            data = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading data from table {self.table}: {e}\")\n",
    "            raise e\n",
    "\n",
    "        return data\n",
    "\n",
    "    def _save(self, data: pd.DataFrame) -> None:\n",
    "        \"\"\"\n",
    "        Save the data to the table.\n",
    "\n",
    "        If unique constraints are violated, updates the conflicting rows with new values for non-conflicting columns.\n",
    "        \"\"\"\n",
    "\n",
    "        verbose = self.save_args.get(\"verbose\", 1)\n",
    "        \n",
    "        try:\n",
    "            # Connect to the database\n",
    "            with psycopg2.connect(self.db_credentials['con']) as conn:\n",
    "                with conn.cursor() as cursor:\n",
    "                    # Prepare data insertion\n",
    "                    for _, row in data.iterrows():\n",
    "                        # Ensure all data is properly converted to standard Python types\n",
    "                        row_data = tuple(\n",
    "                            row[col].item() if isinstance(row[col], (np.generic, np.ndarray)) else row[col]\n",
    "                            for col in self.column_names\n",
    "                        )\n",
    "\n",
    "                        # Determine the update clause (exclude unique columns)\n",
    "                        updatable_columns = [\n",
    "                            col for col in self.column_names if col not in self.unique_columns\n",
    "                        ]\n",
    "\n",
    "                        # Only create an update clause if there are columns to update\n",
    "                        if updatable_columns:\n",
    "                            update_clause = sql.SQL(\", \").join(\n",
    "                                sql.SQL(\"{} = EXCLUDED.{}\").format(\n",
    "                                    sql.Identifier(col), sql.Identifier(col)\n",
    "                                )\n",
    "                                for col in updatable_columns\n",
    "                            )\n",
    "\n",
    "                            # Specify the conflict resolution based on unique columns\n",
    "                            conflict_clause = sql.SQL(\", \").join(\n",
    "                                sql.Identifier(col) for col in self.unique_columns\n",
    "                            )\n",
    "\n",
    "                            # Build the SQL query dynamically\n",
    "                            query = sql.SQL(\"\"\"\n",
    "                                INSERT INTO {table} ({columns})\n",
    "                                VALUES ({values})\n",
    "                                ON CONFLICT ({conflict_clause}) DO UPDATE SET\n",
    "                                    {update_clause}\n",
    "                                RETURNING xmax = 0 AS is_inserted\n",
    "                            \"\"\").format(\n",
    "                                table=sql.Identifier(self.table),\n",
    "                                columns=sql.SQL(\", \").join(sql.Identifier(col) for col in self.column_names),\n",
    "                                values=sql.SQL(\", \").join(sql.Placeholder() for _ in self.column_names),\n",
    "                                conflict_clause=conflict_clause,\n",
    "                                update_clause=update_clause\n",
    "                            )\n",
    "                        else:\n",
    "                            # Build the SQL query for insertion without an update clause\n",
    "                            query = sql.SQL(\"\"\"\n",
    "                                INSERT INTO {table} ({columns})\n",
    "                                VALUES ({values})\n",
    "                                ON CONFLICT DO NOTHING\n",
    "                                RETURNING xmax = 0 AS is_inserted\n",
    "                            \"\"\").format(\n",
    "                                table=sql.Identifier(self.table),\n",
    "                                columns=sql.SQL(\", \").join(sql.Identifier(col) for col in self.column_names),\n",
    "                                values=sql.SQL(\", \").join(sql.Placeholder() for _ in self.column_names)\n",
    "                            )\n",
    "\n",
    "                        # Execute the query with properly cast values\n",
    "                        cursor.execute(query, row_data)\n",
    "\n",
    "                        # Check if the operation was an insert or an update\n",
    "                        is_inserted = cursor.fetchone()[0]\n",
    "\n",
    "                        if verbose > 0:\n",
    "                            if is_inserted:\n",
    "                                logger.info(f\"Inserted new row: {dict(zip(self.column_names, row_data))}\")\n",
    "                            else:\n",
    "                                logger.info(f\"Updated row (or skipped due to conflict): {dict(zip(self.column_names, row_data))}\")\n",
    "\n",
    "                    # Commit the transaction\n",
    "                    conn.commit()\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving data to table {self.table}: {e}\")\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class DynamicPathJSONDataset(AbstractDataset):\n",
    "    \n",
    "    \"\"\"\n",
    "    Custom dataset to dynamically resolve a JSON file path from parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path_param: str):\n",
    "        \n",
    "        \"\"\"\n",
    "        Initializes the ConditionedJSONDataset.\n",
    "        \n",
    "        Args:\n",
    "            path_param (str): The parameter key that contains the file path.\n",
    "        \"\"\"\n",
    "\n",
    "        config_path = str(Path(settings.CONF_SOURCE))\n",
    "        self.config_loader = OmegaConfigLoader(config_path)\n",
    "        self.path_param = path_param\n",
    "\n",
    "    def _load(self) -> dict:\n",
    "        \"\"\"\n",
    "        Loads the JSON data from the file specified in the parameter.\n",
    "        \"\"\"\n",
    "        # Load parameters\n",
    "        params_path = self.config_loader[\"parameters\"][self.path_param]\n",
    "    \n",
    "        # Resolve the file path from parameters\n",
    "        if not params_path:\n",
    "            raise ValueError(f\"Path parameter '{self.path_param}' not found in parameters.\")\n",
    "\n",
    "        # Load and return JSON data\n",
    "        full_path = Path(params_path)\n",
    "        if not full_path.exists():\n",
    "            raise FileNotFoundError(f\"File not found at path: {params_path}\")\n",
    "\n",
    "        with open(params_path, \"r\") as json_file:\n",
    "            return (json.load(json_file), params_path)\n",
    "\n",
    "    def _save(self, data: dict) -> None:\n",
    "        \"\"\"\n",
    "        (Optional) Save method if you want to support writing JSON data.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Saving is not supported for DynamicPathJSONDataset.\")\n",
    "\n",
    "    def _describe(self) -> dict:\n",
    "        \"\"\"\n",
    "        Returns a description of the dataset.\n",
    "        \"\"\"\n",
    "        return {\"path_param\": self.path_param}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
