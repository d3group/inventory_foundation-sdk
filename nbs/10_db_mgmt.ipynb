{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database management\n",
    "\n",
    "> Classes and functions for managing access to the databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp db_mgmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/magnus/miniforge3/envs/inventory_foundation/bin/python\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "\n",
    "from kedro.config import OmegaConfigLoader\n",
    "from kedro.framework.project import settings\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_db_credentials():\n",
    "    \n",
    "    \"\"\"\n",
    "    Fetch PostgreSQL database credentials from the configuration file of the kedro project.\n",
    "\n",
    "    Uses `OmegaConfigLoader` to load credentials stored under `credentials.postgres`.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with the database connection details (e.g., host, port, user, password, dbname).\n",
    "    \"\"\"\n",
    "\n",
    "    conf_path = str(Path(settings.CONF_SOURCE))\n",
    "    conf_loader = OmegaConfigLoader(conf_source=conf_path)\n",
    "    db_credentials = conf_loader[\"credentials\"][\"postgres\"]\n",
    "\n",
    "    return db_credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def insert_multi_rows(\n",
    "    data_to_insert: pd.DataFrame,\n",
    "    table_name: str,\n",
    "    column_names: list,\n",
    "    types: list,\n",
    "    cur,\n",
    "    conn,\n",
    "    return_with_ids: bool = False,\n",
    "    unique_columns: list = None,  # mandatory if return_with_ids is True\n",
    ") -> pd.DataFrame | None:\n",
    "    \n",
    "    \"\"\"\n",
    "    Inserts data into the specified database table, with an optional return of database-assigned IDs.\n",
    "\n",
    "    Args:\n",
    "        data_to_insert (pd.DataFrame): DataFrame containing the data to be inserted.\n",
    "        table_name (str): Name of the target database table.\n",
    "        column_names (list): List of column names for the target table.\n",
    "        types (list): List of Python types (e.g., [int, float]) for data conversion.\n",
    "        cur (psycopg2.cursor): Database cursor for executing SQL commands.\n",
    "        conn (psycopg2.connection): Database connection for committing transactions.\n",
    "        return_with_ids (bool): If True, returns the original DataFrame with an additional \"ID\" column.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame | None: Original DataFrame with an \"ID\" column if `return_with_ids` is True; otherwise, None.\n",
    "    \"\"\"\n",
    "    logger.info(\"-- in insert multi rows -- checking data\")\n",
    "\n",
    "    # Check for NaN values and log a warning if any are found\n",
    "    if data_to_insert.isnull().values.any():\n",
    "        logger.warning(\"There are NaNs in the data\")\n",
    "    \n",
    "    # Ensure the DataFrame has the correct number of columns\n",
    "    if len(column_names) != data_to_insert.shape[1]:\n",
    "        raise ValueError(\"Number of column names does not match the number of columns in the DataFrame.\")\n",
    "    if len(types) != data_to_insert.shape[1]:\n",
    "        raise ValueError(\"Number of types does not match the number of columns in the DataFrame.\")\n",
    "    \n",
    "    logger.info(\"-- in insert multi rows -- converting data to list of tuples\")\n",
    "    # Convert to list of tuples and apply type casting\n",
    "\n",
    "    data_values = data_to_insert.values.tolist()\n",
    "    data_values = [tuple(typ(val) for typ, val in zip(types, row)) for row in data_values]\n",
    "    \n",
    "    logger.info(\"-- in insert multi rows -- preparing SQL\")\n",
    "    # Create SQL placeholders and query\n",
    "    placeholders = \", \".join([\"%s\"] * len(column_names))\n",
    "    column_names_str = \", \".join(f'\"{col}\"' for col in column_names)\n",
    "    \n",
    "\n",
    "    batch_size_for_commit = 1_000_000  # Adjust this based on your dataset size and transaction tolerance\n",
    "    row_count = 0\n",
    "\n",
    "    if return_with_ids:\n",
    "        if not unique_columns:\n",
    "            raise ValueError(\"unique_columns must be provided when return_with_ids is True\")\n",
    "\n",
    "        unique_columns_str = \", \".join(f'\"{col}\"' for col in unique_columns)\n",
    "        insert_query = f\"\"\"\n",
    "            INSERT INTO {table_name} ({column_names_str})\n",
    "            VALUES ({placeholders})\n",
    "            ON CONFLICT ({unique_columns_str})\n",
    "            DO UPDATE SET \"{unique_columns[0]}\" = EXCLUDED.\"{unique_columns[0]}\"\n",
    "            RETURNING \"ID\";\n",
    "        \"\"\"\n",
    "        ids = []\n",
    "\n",
    "        \n",
    "        \n",
    "        # Insert row by row and collect IDs\n",
    "        with tqdm(total=len(data_values), desc=\"Inserting rows\") as pbar:\n",
    "            for row in data_values:\n",
    "                cur.execute(insert_query, row)\n",
    "                row_id = cur.fetchone()\n",
    "                if row_id:\n",
    "                    ids.append(row_id[0])\n",
    "                row_count += 1\n",
    "                pbar.update(1)  # Update progress bar for each row\n",
    "                \n",
    "                # Commit every batch_size_for_commit rows\n",
    "                if row_count % batch_size_for_commit == 0:\n",
    "                    conn.commit()  # Commit the transaction\n",
    "        conn.commit() \n",
    "        \n",
    "        # Add IDs back to the original DataFrame\n",
    "        data_with_ids = data_to_insert.copy()\n",
    "        data_with_ids[\"ID\"] = ids\n",
    "        return data_with_ids\n",
    "\n",
    "    else:\n",
    "        insert_query = f\"\"\"\n",
    "            INSERT INTO {table_name} ({column_names_str})\n",
    "            VALUES ({placeholders})\n",
    "            ON CONFLICT DO NOTHING;\n",
    "        \"\"\"\n",
    "        \n",
    "        # Insert row by row without returning IDs\n",
    "        with tqdm(total=len(data_values), desc=\"Inserting rows\") as pbar:\n",
    "            for row in data_values:\n",
    "                cur.execute(insert_query, row)\n",
    "                row_count += 1\n",
    "                pbar.update(1)  # Update progress bar for each row\n",
    "                if row_count % batch_size_for_commit == 0:\n",
    "                    conn.commit()  # Commit the transaction\n",
    "                \n",
    "        conn.commit()  # Commit all changes after processing\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
