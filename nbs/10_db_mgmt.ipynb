{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database management\n",
    "\n",
    "> Classes and functions for managing access to the databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp db_mgmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/magnus/miniforge3/envs/inventory_foundation/bin/python\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "\n",
    "from kedro.config import OmegaConfigLoader\n",
    "from kedro.framework.project import settings\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import psycopg2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_db_credentials():\n",
    "    \n",
    "    \"\"\"\n",
    "    Fetch PostgreSQL database credentials from the configuration file of the kedro project.\n",
    "\n",
    "    Uses `OmegaConfigLoader` to load credentials stored under `credentials.postgres`.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with the database connection details (e.g., host, port, user, password, dbname).\n",
    "    \"\"\"\n",
    "\n",
    "    conf_path = str(Path(settings.CONF_SOURCE))\n",
    "    conf_loader = OmegaConfigLoader(conf_source=conf_path)\n",
    "    db_credentials = conf_loader[\"credentials\"][\"postgres\"]\n",
    "\n",
    "    return db_credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def insert_multi_rows(\n",
    "    data_to_insert: pd.DataFrame,\n",
    "    table_name: str,\n",
    "    column_names: list,\n",
    "    types: list,\n",
    "    cur,\n",
    "    conn,\n",
    "    return_with_ids: bool = False,\n",
    "    unique_columns: list = None,  # mandatory if return_with_ids is True\n",
    ") -> pd.DataFrame | None:\n",
    "    \n",
    "    \"\"\"\n",
    "    Inserts data into the specified database table, with an optional return of database-assigned IDs.\n",
    "\n",
    "    Args:\n",
    "        data_to_insert (pd.DataFrame): DataFrame containing the data to be inserted.\n",
    "        table_name (str): Name of the target database table.\n",
    "        column_names (list): List of column names for the target table.\n",
    "        types (list): List of Python types (e.g., [int, float]) for data conversion.\n",
    "        cur (psycopg2.cursor): Database cursor for executing SQL commands.\n",
    "        conn (psycopg2.connection): Database connection for committing transactions.\n",
    "        return_with_ids (bool): If True, returns the original DataFrame with an additional \"ID\" column.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame | None: Original DataFrame with an \"ID\" column if `return_with_ids` is True; otherwise, None.\n",
    "    \"\"\"\n",
    "    # logger.info(\"-- in insert multi rows -- checking data\")\n",
    "\n",
    "    # Check for NaN values and log a warning if any are found\n",
    "    if data_to_insert.isnull().values.any():\n",
    "        logger.warning(\"There are NaNs in the data\")\n",
    "    \n",
    "    # Ensure the DataFrame has the correct number of columns\n",
    "    if len(column_names) != data_to_insert.shape[1]:\n",
    "        raise ValueError(\"Number of column names does not match the number of columns in the DataFrame.\")\n",
    "    if len(types) != data_to_insert.shape[1]:\n",
    "        raise ValueError(\"Number of types does not match the number of columns in the DataFrame.\")\n",
    "    \n",
    "    # logger.info(\"-- in insert multi rows -- converting data to list of tuples\")\n",
    "    # Convert to list of tuples and apply type casting\n",
    "\n",
    "    data_values = data_to_insert.values.tolist()\n",
    "    data_values = [tuple(typ(val) for typ, val in zip(types, row)) for row in data_values]\n",
    "    \n",
    "    # logger.info(\"-- in insert multi rows -- preparing SQL\")\n",
    "    # Create SQL placeholders and query\n",
    "    placeholders = \", \".join([\"%s\"] * len(column_names))\n",
    "    column_names_str = \", \".join(f'\"{col}\"' for col in column_names)\n",
    "    \n",
    "\n",
    "    batch_size_for_commit = 1_000_000  # Adjust this based on your dataset size and transaction tolerance\n",
    "    row_count = 0\n",
    "\n",
    "    if return_with_ids:\n",
    "        if not unique_columns:\n",
    "            raise ValueError(\"unique_columns must be provided when return_with_ids is True\")\n",
    "\n",
    "        unique_columns_str = \", \".join(f'\"{col}\"' for col in unique_columns)\n",
    "        insert_query = f\"\"\"\n",
    "            INSERT INTO {table_name} ({column_names_str})\n",
    "            VALUES ({placeholders})\n",
    "            ON CONFLICT ({unique_columns_str})\n",
    "            DO UPDATE SET \"{unique_columns[0]}\" = EXCLUDED.\"{unique_columns[0]}\"\n",
    "            RETURNING \"ID\";\n",
    "        \"\"\"\n",
    "        ids = []\n",
    "\n",
    "        \n",
    "        \n",
    "        # Insert row by row and collect IDs\n",
    "        with tqdm(total=len(data_values), desc=\"Inserting rows\") as pbar:\n",
    "            for row in data_values:\n",
    "                cur.execute(insert_query, row)\n",
    "                row_id = cur.fetchone()\n",
    "                if row_id:\n",
    "                    ids.append(row_id[0])\n",
    "                row_count += 1\n",
    "                pbar.update(1)  # Update progress bar for each row\n",
    "                \n",
    "                # Commit every batch_size_for_commit rows\n",
    "                if row_count % batch_size_for_commit == 0:\n",
    "                    conn.commit()  # Commit the transaction\n",
    "        conn.commit() \n",
    "        \n",
    "        # Add IDs back to the original DataFrame\n",
    "        data_with_ids = data_to_insert.copy()\n",
    "        data_with_ids[\"ID\"] = ids\n",
    "        return data_with_ids\n",
    "\n",
    "    else:\n",
    "        insert_query = f\"\"\"\n",
    "            INSERT INTO {table_name} ({column_names_str})\n",
    "            VALUES ({placeholders})\n",
    "            ON CONFLICT DO NOTHING;\n",
    "        \"\"\"\n",
    "        \n",
    "        # Insert row by row without returning IDs\n",
    "        with tqdm(total=len(data_values), desc=\"Inserting rows\") as pbar:\n",
    "            for row in data_values:\n",
    "                cur.execute(insert_query, row)\n",
    "                row_count += 1\n",
    "                pbar.update(1)  # Update progress bar for each row\n",
    "                if row_count % batch_size_for_commit == 0:\n",
    "                    conn.commit()  # Commit the transaction\n",
    "                \n",
    "        conn.commit()  # Commit all changes after processing\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def check_in_scope_entries(\n",
    "    target_table,\n",
    "    dataset_column,\n",
    "    id_column,\n",
    "    insert_arguments,\n",
    "    credentials,\n",
    "    dataset_id,\n",
    "    logger,\n",
    "    filter=None,\n",
    "    further_primary_keys=None,\n",
    "    further_primary_keys_values=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Ensures all entries in the dataset scope have corresponding entries in the target table.\n",
    "    If an entry is missing, specified insert_arguments are set to zero.\n",
    "\n",
    "    Args:\n",
    "        target_table (str): The name of the target table to check and update.\n",
    "        dataset_column (str): The name of the column identifying the dataset (e.g., \"datasetID\").\n",
    "        id_column (str): The name of the column identifying the entries (e.g., \"skuID\").\n",
    "        insert_arguments (list): List of columns to be inserted with default zero values if missing.\n",
    "        further_primary_keys (list): Additional primary key columns in the target table.\n",
    "        further_primary_keys_values (list): Corresponding values for further_primary_keys.\n",
    "    \"\"\"\n",
    "    if not further_primary_keys or not further_primary_keys_values:\n",
    "        raise ValueError(\"Both further_primary_keys and further_primary_keys_values must be provided together.\")\n",
    "\n",
    "    try:\n",
    "        with psycopg2.connect(credentials) as conn:\n",
    "            with conn.cursor() as cur:\n",
    "                # Step 1: Get all `skuIDs` in the dataset scope\n",
    "                cur.execute(f\"\"\"\n",
    "                    SELECT \"{id_column}\"\n",
    "                    FROM dataset_matching\n",
    "                    WHERE \"{dataset_column}\" = %s;\n",
    "                \"\"\", (dataset_id,))\n",
    "                all_sku_ids = {row[0] for row in cur.fetchall()}\n",
    "\n",
    "                # Step 2: Get `skuIDs` from the target table matching further_primary_keys_values\n",
    "                additional_conditions = \" AND \".join(\n",
    "                    f'\"{key}\" = %s' for key in further_primary_keys\n",
    "                )\n",
    "                additional_values = further_primary_keys_values\n",
    "\n",
    "                cur.execute(f\"\"\"\n",
    "                    SELECT DISTINCT \"{id_column}\"\n",
    "                    FROM {target_table}\n",
    "                    WHERE \"{dataset_column}\" = %s\n",
    "                    AND {additional_conditions};\n",
    "                \"\"\", (dataset_id, *additional_values))\n",
    "                existing_sku_ids = {row[0] for row in cur.fetchall()}\n",
    "\n",
    "                # Step 3: Identify missing `skuIDs`\n",
    "                missing_sku_ids = all_sku_ids - existing_sku_ids\n",
    "\n",
    "                # Step 4: Insert missing rows\n",
    "                if missing_sku_ids:\n",
    "                    logger.info(f\"Adding missing IDs for {target_table}: {missing_sku_ids}\")\n",
    "\n",
    "                    # Build column list and placeholders\n",
    "                    columns = [id_column, dataset_column] + insert_arguments + further_primary_keys\n",
    "                    column_list = \", \".join(f'\"{col}\"' for col in columns)\n",
    "                    value_placeholders = [\"%s\", \"%s\"] + [\"0\"] * len(insert_arguments) + [\"%s\"] * len(further_primary_keys)\n",
    "\n",
    "                    for sku_id in missing_sku_ids:\n",
    "                        args = (sku_id, dataset_id, *further_primary_keys_values)\n",
    "                        query = f\"\"\"\n",
    "                            INSERT INTO {target_table} ({column_list})\n",
    "                            VALUES ({\", \".join(value_placeholders)})\n",
    "                            ON CONFLICT ({\", \".join(f'\"{col}\"' for col in [id_column, dataset_column] + further_primary_keys)})\n",
    "                            DO NOTHING;\n",
    "                        \"\"\"\n",
    "                        cur.execute(query, args)\n",
    "                    \n",
    "                    conn.commit()\n",
    "                    logger.info(f\"Missing IDs handled successfully for {target_table}.\")\n",
    "                else:\n",
    "                    logger.info(\"No missing IDs to handle.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error checking in-scope entries for {target_table}: {e}\")\n",
    "        raise e\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
