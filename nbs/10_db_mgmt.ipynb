{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database management\n",
    "\n",
    "> Classes and functions for managing access to the databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp db_mgmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/magnus/miniforge3/envs/inventory_foundation/bin/python\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "\n",
    "from kedro.config import OmegaConfigLoader\n",
    "from kedro.framework.project import settings\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_db_credentials():\n",
    "    \n",
    "    \"\"\"\n",
    "    Fetch PostgreSQL database credentials from the configuration file of the kedro project.\n",
    "\n",
    "    Uses `OmegaConfigLoader` to load credentials stored under `credentials.postgres`.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with the database connection details (e.g., host, port, user, password, dbname).\n",
    "    \"\"\"\n",
    "\n",
    "    conf_path = str(Path(settings.CONF_SOURCE))\n",
    "    conf_loader = OmegaConfigLoader(conf_source=conf_path)\n",
    "    db_credentials = conf_loader[\"credentials\"][\"postgres\"]\n",
    "\n",
    "    return db_credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def insert_multi_rows(\n",
    "    data_to_insert: pd.DataFrame,\n",
    "    table_name: str,\n",
    "    column_names: list,\n",
    "    types: list,\n",
    "    cur,\n",
    "    conn,\n",
    "    return_with_ids: bool = False,\n",
    "    unique_columns: list = None,  # mandatory if return_with_ids is True\n",
    ") -> pd.DataFrame | None:\n",
    "    \n",
    "    \"\"\"\n",
    "    Inserts data into the specified database table, with an optional return of database-assigned IDs.\n",
    "\n",
    "    Args:\n",
    "        data_to_insert (pd.DataFrame): DataFrame containing the data to be inserted.\n",
    "        table_name (str): Name of the target database table.\n",
    "        column_names (list): List of column names for the target table.\n",
    "        types (list): List of Python types (e.g., [int, float]) for data conversion.\n",
    "        cur (psycopg2.cursor): Database cursor for executing SQL commands.\n",
    "        conn (psycopg2.connection): Database connection for committing transactions.\n",
    "        return_with_ids (bool): If True, returns the original DataFrame with an additional \"ID\" column.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame | None: Original DataFrame with an \"ID\" column if `return_with_ids` is True; otherwise, None.\n",
    "    \"\"\"\n",
    "    logger.info(\"-- in insert multi rows -- checking data\")\n",
    "\n",
    "    # Check for NaN values and log a warning if any are found\n",
    "    if data_to_insert.isnull().values.any():\n",
    "        logger.warning(\"There are NaNs in the data\")\n",
    "    \n",
    "    # Ensure the DataFrame has the correct number of columns\n",
    "    if len(column_names) != data_to_insert.shape[1]:\n",
    "        raise ValueError(\"Number of column names does not match the number of columns in the DataFrame.\")\n",
    "    if len(types) != data_to_insert.shape[1]:\n",
    "        raise ValueError(\"Number of types does not match the number of columns in the DataFrame.\")\n",
    "    \n",
    "    logger.info(\"-- in insert multi rows -- converting data to list of tuples\")\n",
    "    # Convert to list of tuples and apply type casting\n",
    "\n",
    "    data_values = data_to_insert.values.tolist()\n",
    "    data_values = [tuple(typ(val) for typ, val in zip(types, row)) for row in data_values]\n",
    "    \n",
    "    logger.info(\"-- in insert multi rows -- preparing SQL\")\n",
    "    # Create SQL placeholders and query\n",
    "    placeholders = \", \".join([\"%s\"] * len(column_names))\n",
    "    column_names_str = \", \".join(f'\"{col}\"' for col in column_names)\n",
    "    \n",
    "\n",
    "    batch_size_for_commit = 1_000_000  # Adjust this based on your dataset size and transaction tolerance\n",
    "    row_count = 0\n",
    "\n",
    "    if return_with_ids:\n",
    "        if not unique_columns:\n",
    "            raise ValueError(\"unique_columns must be provided when return_with_ids is True\")\n",
    "\n",
    "        unique_columns_str = \", \".join(f'\"{col}\"' for col in unique_columns)\n",
    "        insert_query = f\"\"\"\n",
    "            INSERT INTO {table_name} ({column_names_str})\n",
    "            VALUES ({placeholders})\n",
    "            ON CONFLICT ({unique_columns_str})\n",
    "            DO UPDATE SET \"{unique_columns[0]}\" = EXCLUDED.\"{unique_columns[0]}\"\n",
    "            RETURNING \"ID\";\n",
    "        \"\"\"\n",
    "        ids = []\n",
    "\n",
    "        \n",
    "        \n",
    "        # Insert row by row and collect IDs\n",
    "        with tqdm(total=len(data_values), desc=\"Inserting rows\") as pbar:\n",
    "            for row in data_values:\n",
    "                cur.execute(insert_query, row)\n",
    "                row_id = cur.fetchone()\n",
    "                if row_id:\n",
    "                    ids.append(row_id[0])\n",
    "                row_count += 1\n",
    "                pbar.update(1)  # Update progress bar for each row\n",
    "                \n",
    "                # Commit every batch_size_for_commit rows\n",
    "                if row_count % batch_size_for_commit == 0:\n",
    "                    conn.commit()  # Commit the transaction\n",
    "        conn.commit() \n",
    "        \n",
    "        # Add IDs back to the original DataFrame\n",
    "        data_with_ids = data_to_insert.copy()\n",
    "        data_with_ids[\"ID\"] = ids\n",
    "        return data_with_ids\n",
    "\n",
    "    else:\n",
    "        insert_query = f\"\"\"\n",
    "            INSERT INTO {table_name} ({column_names_str})\n",
    "            VALUES ({placeholders})\n",
    "            ON CONFLICT DO NOTHING;\n",
    "        \"\"\"\n",
    "        \n",
    "        # Insert row by row without returning IDs\n",
    "        with tqdm(total=len(data_values), desc=\"Inserting rows\") as pbar:\n",
    "            for row in data_values:\n",
    "                cur.execute(insert_query, row)\n",
    "                row_count += 1\n",
    "                pbar.update(1)  # Update progress bar for each row\n",
    "                if row_count % batch_size_for_commit == 0:\n",
    "                    conn.commit()  # Commit the transaction\n",
    "                \n",
    "        conn.commit()  # Commit all changes after processing\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def check_in_scope_entries(target_table, dataset_column, id_column, insert_arguments, credentials, dataset_id, filter=None):\n",
    "    \n",
    "    \"\"\"\n",
    "    Ensures all entries in the dataset scope have corresponding entries in the target table.\n",
    "    If an entry is missing, specified insert_arguments are set to zero.\n",
    "\n",
    "    Args:\n",
    "        target_table (str): The name of the target table to check and update.\n",
    "        dataset_column (str): The name of the column identifying the dataset (e.g., \"datasetID\").\n",
    "        id_column (str): The name of the column identifying the entries (e.g., \"skuID\").\n",
    "        insert_arguments (list): List of columns to be inserted with default zero values if missing.\n",
    "        filter (list, optional): List of dictionaries specifying filter conditions. \n",
    "                                    Each dictionary contains:\n",
    "                                        - \"column\" (str): Column name for the condition.\n",
    "                                        - \"value\" (any): Value for the condition.\n",
    "    \"\"\"\n",
    "    if filter is None:\n",
    "        logger.info(f\"Checking in-scope entries for {target_table}...\")\n",
    "    else:\n",
    "        logger.info(f\"Checking in-scope entries for {target_table} with filter: {filter}\")\n",
    "\n",
    "    try:\n",
    "        with psycopg2.connect(credentials) as conn:\n",
    "            with conn.cursor() as cur:\n",
    "                # Get all IDs in the dataset scope (no filter clause here)\n",
    "                cur.execute(f\"\"\"\n",
    "                    SELECT \"{id_column}\"\n",
    "                    FROM dataset_matching\n",
    "                    WHERE \"{dataset_column}\" = %s;\n",
    "                \"\"\", (dataset_id,))\n",
    "                all_ids = {row[0] for row in cur.fetchall()}\n",
    "\n",
    "                # Build filter SQL for WHERE clause if provided\n",
    "                filter_clause = \"\"\n",
    "                filter_values = []\n",
    "                if filter:\n",
    "                    filter_conditions = [f'\"{f[\"column\"]}\" = %s' for f in filter]\n",
    "                    filter_clause = \" AND \" + \" AND \".join(filter_conditions)\n",
    "                    filter_values = [f[\"value\"] for f in filter]\n",
    "\n",
    "                # Get IDs already present in the target table (apply filter here)\n",
    "                cur.execute(f\"\"\"\n",
    "                    SELECT DISTINCT \"{id_column}\"\n",
    "                    FROM {target_table}\n",
    "                    WHERE \"{dataset_column}\" = %s {filter_clause};\n",
    "                \"\"\", (dataset_id, *filter_values))\n",
    "                existing_ids = {row[0] for row in cur.fetchall()}\n",
    "\n",
    "                # Identify IDs with no entries in the target table\n",
    "                missing_ids = all_ids - existing_ids\n",
    "\n",
    "                if missing_ids:\n",
    "                    if filter is None:\n",
    "                        logger.info(f\"Adding missing IDs with zero values for {target_table}: {missing_ids}\")\n",
    "                    else:\n",
    "                        logger.info(f\"Adding missing IDs with zero values for {target_table} with filter {filter}: {missing_ids}\")\n",
    "\n",
    "                    # Build column list and value placeholders\n",
    "                    columns = [id_column, dataset_column] + insert_arguments\n",
    "                    if filter:\n",
    "                        columns += [f[\"column\"] for f in filter]\n",
    "                    column_list = \", \".join(f'\"{col}\"' for col in columns)\n",
    "\n",
    "                    value_placeholders = [\"%s\", \"%s\"] + [\"0\"] * len(insert_arguments)\n",
    "                    if filter:\n",
    "                        value_placeholders += [\"%s\"] * len(filter)\n",
    "                    value_placeholder_list = \", \".join(value_placeholders)\n",
    "\n",
    "                    # Insert zero values for missing IDs\n",
    "                    for entry_id in missing_ids:\n",
    "                        cur.execute(f\"\"\"\n",
    "                            INSERT INTO {target_table} ({column_list})\n",
    "                            VALUES ({value_placeholder_list})\n",
    "                            ON CONFLICT ({\", \".join(f'\"{col}\"' for col in [id_column, dataset_column] + ([f[\"column\"] for f in filter] if filter else []))})\n",
    "                            DO NOTHING;\n",
    "                        \"\"\", (entry_id, dataset_id, *filter_values))\n",
    "                    \n",
    "                    conn.commit()\n",
    "                    logger.info(f\"Missing IDs handled successfully for {target_table}.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error checking in-scope entries for {target_table}: {e}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
