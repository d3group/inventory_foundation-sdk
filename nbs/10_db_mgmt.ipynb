{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database management\n",
    "\n",
    "> Classes and functions for managing access to the databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp db_mgmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[01/28/25 09:45:11] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Using                                                                  <a href=\"file:///Users/moritzbeckmail.de/miniconda3/envs/if_sdk/lib/python3.11/site-packages/kedro/framework/project/__init__.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">__init__.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/moritzbeckmail.de/miniconda3/envs/if_sdk/lib/python3.11/site-packages/kedro/framework/project/__init__.py#270\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">270</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">'/Users/moritzbeckmail.de/miniconda3/envs/if_sdk/lib/python3.11/site-p</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">ackages/kedro/framework/project/rich_logging.yml'</span> as logging           <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         configuration.                                                         <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[01/28/25 09:45:11]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Using                                                                  \u001b]8;id=300718;file:///Users/moritzbeckmail.de/miniconda3/envs/if_sdk/lib/python3.11/site-packages/kedro/framework/project/__init__.py\u001b\\\u001b[2m__init__.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=980124;file:///Users/moritzbeckmail.de/miniconda3/envs/if_sdk/lib/python3.11/site-packages/kedro/framework/project/__init__.py#270\u001b\\\u001b[2m270\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32m'/Users/moritzbeckmail.de/miniconda3/envs/if_sdk/lib/python3.11/site-p\u001b[0m \u001b[2m               \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32mackages/kedro/framework/project/rich_logging.yml'\u001b[0m as logging           \u001b[2m               \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         configuration.                                                         \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| export\n",
    "\n",
    "from kedro.config import OmegaConfigLoader\n",
    "from kedro.framework.project import settings\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import psycopg2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_db_credentials():\n",
    "    \n",
    "    \"\"\"\n",
    "    Fetch PostgreSQL database credentials from the configuration file of the kedro project.\n",
    "\n",
    "    Uses `OmegaConfigLoader` to load credentials stored under `credentials.postgres`.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with the database connection details (e.g., host, port, user, password, dbname).\n",
    "    \"\"\"\n",
    "\n",
    "    conf_path = str(Path(settings.CONF_SOURCE))\n",
    "    conf_loader = OmegaConfigLoader(conf_source=conf_path)\n",
    "    db_credentials = conf_loader[\"credentials\"][\"postgres\"]\n",
    "\n",
    "    return db_credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def insert_multi_rows(\n",
    "    data_to_insert: pd.DataFrame,\n",
    "    table_name: str,\n",
    "    column_names: list,\n",
    "    types: list,\n",
    "    cur,\n",
    "    conn,\n",
    "    return_with_ids: bool = False,\n",
    "    unique_columns: list = None,  # mandatory if return_with_ids is True\n",
    ") -> pd.DataFrame | None:\n",
    "    \n",
    "    \"\"\"\n",
    "    Inserts data into the specified database table, with an optional return of database-assigned IDs.\n",
    "\n",
    "    Args:\n",
    "        data_to_insert (pd.DataFrame): DataFrame containing the data to be inserted.\n",
    "        table_name (str): Name of the target database table.\n",
    "        column_names (list): List of column names for the target table.\n",
    "        types (list): List of Python types (e.g., [int, float]) for data conversion.\n",
    "        cur (psycopg2.cursor): Database cursor for executing SQL commands.\n",
    "        conn (psycopg2.connection): Database connection for committing transactions.\n",
    "        return_with_ids (bool): If True, returns the original DataFrame with an additional \"ID\" column.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame | None: Original DataFrame with an \"ID\" column if `return_with_ids` is True; otherwise, None.\n",
    "    \"\"\"\n",
    "    # logger.info(\"-- in insert multi rows -- checking data\")\n",
    "\n",
    "    # Check for NaN values and log a warning if any are found\n",
    "    if data_to_insert.isnull().values.any():\n",
    "        logger.warning(\"There are NaNs in the data\")\n",
    "    \n",
    "    # Ensure the DataFrame has the correct number of columns\n",
    "    if len(column_names) != data_to_insert.shape[1]:\n",
    "        raise ValueError(\"Number of column names does not match the number of columns in the DataFrame.\")\n",
    "    if len(types) != data_to_insert.shape[1]:\n",
    "        raise ValueError(\"Number of types does not match the number of columns in the DataFrame.\")\n",
    "    \n",
    "    # logger.info(\"-- in insert multi rows -- converting data to list of tuples\")\n",
    "    # Convert to list of tuples and apply type casting\n",
    "\n",
    "    data_values = data_to_insert.values.tolist()\n",
    "    data_values = [tuple(typ(val) for typ, val in zip(types, row)) for row in data_values]\n",
    "    \n",
    "    # logger.info(\"-- in insert multi rows -- preparing SQL\")\n",
    "    # Create SQL placeholders and query\n",
    "    placeholders = \", \".join([\"%s\"] * len(column_names))\n",
    "    column_names_str = \", \".join(f'\"{col}\"' for col in column_names)\n",
    "    \n",
    "\n",
    "    batch_size_for_commit = 1_000_000  # Adjust this based on your dataset size and transaction tolerance\n",
    "    row_count = 0\n",
    "\n",
    "    if return_with_ids:\n",
    "        if not unique_columns:\n",
    "            raise ValueError(\"unique_columns must be provided when return_with_ids is True\")\n",
    "\n",
    "        unique_columns_str = \", \".join(f'\"{col}\"' for col in unique_columns)\n",
    "        insert_query = f\"\"\"\n",
    "            INSERT INTO {table_name} ({column_names_str})\n",
    "            VALUES ({placeholders})\n",
    "            ON CONFLICT ({unique_columns_str})\n",
    "            DO UPDATE SET \"{unique_columns[0]}\" = EXCLUDED.\"{unique_columns[0]}\"\n",
    "            RETURNING \"ID\";\n",
    "        \"\"\"\n",
    "        ids = []\n",
    "\n",
    "        \n",
    "        \n",
    "        # Insert row by row and collect IDs\n",
    "        with tqdm(total=len(data_values), desc=\"Inserting rows\") as pbar:\n",
    "            for row in data_values:\n",
    "                cur.execute(insert_query, row)\n",
    "                row_id = cur.fetchone()\n",
    "                if row_id:\n",
    "                    ids.append(row_id[0])\n",
    "                row_count += 1\n",
    "                pbar.update(1)  # Update progress bar for each row\n",
    "                \n",
    "                # Commit every batch_size_for_commit rows\n",
    "                if row_count % batch_size_for_commit == 0:\n",
    "                    conn.commit()  # Commit the transaction\n",
    "        conn.commit() \n",
    "        \n",
    "        # Add IDs back to the original DataFrame\n",
    "        data_with_ids = data_to_insert.copy()\n",
    "        data_with_ids[\"ID\"] = ids\n",
    "        return data_with_ids\n",
    "\n",
    "    else:\n",
    "        insert_query = f\"\"\"\n",
    "            INSERT INTO {table_name} ({column_names_str})\n",
    "            VALUES ({placeholders})\n",
    "            ON CONFLICT DO NOTHING;\n",
    "        \"\"\"\n",
    "        \n",
    "        # Insert row by row without returning IDs\n",
    "        with tqdm(total=len(data_values), desc=\"Inserting rows\") as pbar:\n",
    "            for row in data_values:\n",
    "                cur.execute(insert_query, row)\n",
    "                row_count += 1\n",
    "                pbar.update(1)  # Update progress bar for each row\n",
    "                if row_count % batch_size_for_commit == 0:\n",
    "                    conn.commit()  # Commit the transaction\n",
    "                \n",
    "        conn.commit()  # Commit all changes after processing\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def check_in_scope_entries(\n",
    "    target_table,\n",
    "    dataset_column,\n",
    "    id_column,\n",
    "    insert_arguments,\n",
    "    credentials,\n",
    "    dataset_id,\n",
    "    logger,\n",
    "    filter=None,\n",
    "    further_primary_keys=None,\n",
    "    further_primary_keys_values=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Ensures all entries in the dataset scope have corresponding entries in the target table.\n",
    "    If an entry is missing, specified insert_arguments are set to zero.\n",
    "\n",
    "    Args:\n",
    "        target_table (str): The name of the target table to check and update.\n",
    "        dataset_column (str): The name of the column identifying the dataset (e.g., \"datasetID\").\n",
    "        id_column (str): The name of the column identifying the entries (e.g., \"skuID\").\n",
    "        insert_arguments (list): List of columns to be inserted with default zero values if missing.\n",
    "        further_primary_keys (list): Additional primary key columns in the target table.\n",
    "        further_primary_keys_values (list): Corresponding values for further_primary_keys.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if one is provided without the other\n",
    "    if (further_primary_keys is None) != (further_primary_keys_values is None):\n",
    "        raise ValueError(\"Both further_primary_keys and further_primary_keys_values must be provided together.\")\n",
    "\n",
    "    try:\n",
    "        with psycopg2.connect(credentials) as conn:\n",
    "            with conn.cursor() as cur:\n",
    "                # Step 1: Get all `skuIDs` in the dataset scope\n",
    "                cur.execute(f\"\"\"\n",
    "                    SELECT \"{id_column}\"\n",
    "                    FROM dataset_matching\n",
    "                    WHERE \"{dataset_column}\" = %s;\n",
    "                \"\"\", (dataset_id,))\n",
    "                all_sku_ids = {row[0] for row in cur.fetchall()}\n",
    "\n",
    "                # Step 2: Build query for `skuIDs` in the target table\n",
    "                where_clause = f'\"{dataset_column}\" = %s'\n",
    "                query_params = [dataset_id]\n",
    "\n",
    "                if further_primary_keys:\n",
    "                    additional_conditions = \" AND \".join(\n",
    "                        f'\"{key}\" = %s' for key in further_primary_keys\n",
    "                    )\n",
    "                    where_clause += f\" AND {additional_conditions}\"\n",
    "                    query_params.extend(further_primary_keys_values)\n",
    "\n",
    "                cur.execute(f\"\"\"\n",
    "                    SELECT DISTINCT \"{id_column}\"\n",
    "                    FROM {target_table}\n",
    "                    WHERE {where_clause};\n",
    "                \"\"\", query_params)\n",
    "                existing_sku_ids = {row[0] for row in cur.fetchall()}\n",
    "\n",
    "                # Step 3: Identify missing `skuIDs`\n",
    "                missing_sku_ids = all_sku_ids - existing_sku_ids\n",
    "\n",
    "                if missing_sku_ids:\n",
    "                    logger.info(f\"Adding missing IDs for {target_table}: {missing_sku_ids}\")\n",
    "\n",
    "                    # Build column list and placeholders\n",
    "                    columns = [id_column, dataset_column] + insert_arguments\n",
    "                    value_placeholders = [\"%s\", \"%s\"] + [\"0\"] * len(insert_arguments)\n",
    "\n",
    "                    if further_primary_keys:\n",
    "                        columns += further_primary_keys\n",
    "                        value_placeholders += [\"%s\"] * len(further_primary_keys)\n",
    "\n",
    "                    column_list = \", \".join(f'\"{col}\"' for col in columns)\n",
    "                    placeholder_list = \", \".join(value_placeholders)\n",
    "\n",
    "                    for sku_id in missing_sku_ids:\n",
    "                        # Build arguments dynamically\n",
    "                        args = [sku_id, dataset_id]\n",
    "                        if further_primary_keys_values:\n",
    "                            args += further_primary_keys_values\n",
    "\n",
    "                        query = f\"\"\"\n",
    "                            INSERT INTO {target_table} ({column_list})\n",
    "                            VALUES ({placeholder_list})\n",
    "                            ON CONFLICT ({\", \".join(f'\"{col}\"' for col in [id_column, dataset_column] + (further_primary_keys or []))})\n",
    "                            DO NOTHING;\n",
    "                        \"\"\"\n",
    "\n",
    "                        # Debugging information\n",
    "                        # print(f\"Executing query: {query}\")\n",
    "                        # print(f\"With arguments: {args}\")\n",
    "\n",
    "                        # Execute the query\n",
    "                        cur.execute(query, args)\n",
    "\n",
    "                    conn.commit()\n",
    "                    logger.info(f\"Missing IDs handled successfully for {target_table}.\")\n",
    "                else:\n",
    "                    logger.info(\"No missing IDs to handle.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error checking in-scope entries for {target_table}: {e}\")\n",
    "        raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def insert_multi_rows(\n",
    "    data_to_insert: pd.DataFrame,\n",
    "    table_name: str,\n",
    "    column_names: list,\n",
    "    types: list,\n",
    "    cur,\n",
    "    conn,\n",
    "    return_with_ids: bool = False,\n",
    "    unique_columns: list = None,  # mandatory if return_with_ids is True\n",
    ") -> pd.DataFrame | None:\n",
    "    \n",
    "    \"\"\"\n",
    "    Inserts data into the specified database table, with an optional return of database-assigned IDs.\n",
    "\n",
    "    Args:\n",
    "        data_to_insert (pd.DataFrame): DataFrame containing the data to be inserted.\n",
    "        table_name (str): Name of the target database table.\n",
    "        column_names (list): List of column names for the target table.\n",
    "        types (list): List of Python types (e.g., [int, float]) for data conversion.\n",
    "        cur (psycopg2.cursor): Database cursor for executing SQL commands.\n",
    "        conn (psycopg2.connection): Database connection for committing transactions.\n",
    "        return_with_ids (bool): If True, returns the original DataFrame with an additional \"ID\" column.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame | None: Original DataFrame with an \"ID\" column if `return_with_ids` is True; otherwise, None.\n",
    "    \"\"\"\n",
    "    # logger.info(\"-- in insert multi rows -- checking data\")\n",
    "\n",
    "    # Check for NaN values and log a warning if any are found\n",
    "    if data_to_insert.isnull().values.any():\n",
    "        logger.warning(\"There are NaNs in the data\")\n",
    "    \n",
    "    # Ensure the DataFrame has the correct number of columns\n",
    "    if len(column_names) != data_to_insert.shape[1]:\n",
    "        raise ValueError(\"Number of column names does not match the number of columns in the DataFrame.\")\n",
    "    if len(types) != data_to_insert.shape[1]:\n",
    "        raise ValueError(\"Number of types does not match the number of columns in the DataFrame.\")\n",
    "    \n",
    "    # logger.info(\"-- in insert multi rows -- converting data to list of tuples\")\n",
    "    # Convert to list of tuples and apply type casting\n",
    "\n",
    "    data_values = data_to_insert.values.tolist()\n",
    "    data_values = [tuple(typ(val) for typ, val in zip(types, row)) for row in data_values]\n",
    "    \n",
    "    # logger.info(\"-- in insert multi rows -- preparing SQL\")\n",
    "    # Create SQL placeholders and query\n",
    "    placeholders = \", \".join([\"%s\"] * len(column_names))\n",
    "    column_names_str = \", \".join(f'\"{col}\"' for col in column_names)\n",
    "    \n",
    "\n",
    "    batch_size_for_commit = 1_000_000  # Adjust this based on your dataset size and transaction tolerance\n",
    "    row_count = 0\n",
    "\n",
    "    if return_with_ids:\n",
    "        if not unique_columns:\n",
    "            raise ValueError(\"unique_columns must be provided when return_with_ids is True\")\n",
    "\n",
    "        unique_columns_str = \", \".join(f'\"{col}\"' for col in unique_columns)\n",
    "        insert_query = f\"\"\"\n",
    "            INSERT INTO {table_name} ({column_names_str})\n",
    "            VALUES ({placeholders})\n",
    "            ON CONFLICT ({unique_columns_str})\n",
    "            DO UPDATE SET \"{unique_columns[0]}\" = EXCLUDED.\"{unique_columns[0]}\"\n",
    "            RETURNING \"ID\";\n",
    "        \"\"\"\n",
    "        ids = []\n",
    "\n",
    "        \n",
    "        \n",
    "        # Insert row by row and collect IDs\n",
    "        with tqdm(total=len(data_values), desc=\"Inserting rows\") as pbar:\n",
    "            for row in data_values:\n",
    "                cur.execute(insert_query, row)\n",
    "                row_id = cur.fetchone()\n",
    "                if row_id:\n",
    "                    ids.append(row_id[0])\n",
    "                row_count += 1\n",
    "                pbar.update(1)  # Update progress bar for each row\n",
    "                \n",
    "                # Commit every batch_size_for_commit rows\n",
    "                if row_count % batch_size_for_commit == 0:\n",
    "                    conn.commit()  # Commit the transaction\n",
    "        conn.commit() \n",
    "        \n",
    "        # Add IDs back to the original DataFrame\n",
    "        data_with_ids = data_to_insert.copy()\n",
    "        data_with_ids[\"ID\"] = ids\n",
    "        return data_with_ids\n",
    "\n",
    "    else:\n",
    "        insert_query = f\"\"\"\n",
    "            INSERT INTO {table_name} ({column_names_str})\n",
    "            VALUES ({placeholders})\n",
    "            ON CONFLICT DO NOTHING;\n",
    "        \"\"\"\n",
    "        \n",
    "        # Insert row by row without returning IDs\n",
    "        with tqdm(total=len(data_values), desc=\"Inserting rows\") as pbar:\n",
    "            for row in data_values:\n",
    "                cur.execute(insert_query, row)\n",
    "                row_count += 1\n",
    "                pbar.update(1)  # Update progress bar for each row\n",
    "                if row_count % batch_size_for_commit == 0:\n",
    "                    conn.commit()  # Commit the transaction\n",
    "                \n",
    "        conn.commit()  # Commit all changes after processing\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class SQLDatabase:\n",
    "    \"\"\"\n",
    "    A class to represent a SQL database.\n",
    "\n",
    "    Attributes:\n",
    "    ----------\n",
    "    credentials : dict\n",
    "        A dictionary containing the database connection credentials.\n",
    "    connection : psycopg2.connection\n",
    "        The database connection object.\n",
    "\n",
    "    Methods:\n",
    "    -------\n",
    "    connect():\n",
    "        Connects to the database using the provided credentials.\n",
    "    \n",
    "    close():\n",
    "        Closes the database connection.\n",
    "    \n",
    "    execute_query(query: str):\n",
    "        Executes the given SQL query.\n",
    "    \n",
    "    execute_multiple_queries(queries: list, params: list = None, fetchrows: bool = False):\n",
    "        Executes a list or iterable of SQL queries. \n",
    "        If fetchrows is True, iterates over queries and fetches rows.\n",
    "        If fetchrows is False, uses executemany for batch execution.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the SQLDatabase object with the provided database connection credentials.\n",
    "        \"\"\"\n",
    "        self._credentials = get_db_credentials()[\"con\"]\n",
    "        self.connection = None\n",
    "\n",
    "    def connect(self):\n",
    "        \"\"\"\n",
    "        Connects to the database using the provided credentials.\n",
    "        \"\"\"\n",
    "        if not self.connection:\n",
    "            self.connection = psycopg2.connect(self._credentials)\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Closes the database connection.\n",
    "        \"\"\"\n",
    "        if self.connection:\n",
    "            self.connection.close()\n",
    "            self.connection = None\n",
    "\n",
    "    def execute_query(self, query: str, params: tuple = None, fetchall: bool = False, fetchone = False):\n",
    "        \"\"\"\n",
    "        Executes the given SQL query with optional parameters.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        query : str\n",
    "            The SQL query to be executed.\n",
    "        \n",
    "        params : tuple, optional\n",
    "            The parameters to be passed to the query. Defaults to None.\n",
    "\n",
    "        fetchall : bool, optional\n",
    "            Whether to fetch all rows from the query result. Defaults to False.\n",
    "\n",
    "        fetchone : bool, optional\n",
    "            Whether to fetch only one row from the query result. Defaults to False.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        result : list\n",
    "            A list of tuples representing the query result rows.\n",
    "        \"\"\"\n",
    "\n",
    "        # Check if not both fetchall and fetchone are True\n",
    "        if fetchall and fetchone:\n",
    "            raise ValueError(\"Both fetchall and fetchone cannot be True\")\n",
    "\n",
    "        if not self.connection:\n",
    "            self.connect()\n",
    "\n",
    "        with self.connection.cursor() as cur:\n",
    "            cur.execute(query, params)\n",
    "            if fetchall:\n",
    "                result = cur.fetchall()\n",
    "            elif fetchone:\n",
    "                result = cur.fetchone()\n",
    "            else:\n",
    "                result = None\n",
    "\n",
    "        self.connection.commit()\n",
    "        return result\n",
    "    \n",
    "\n",
    "    def execute_multiple_queries(self, queries: list | str, params: list = None, fetchrows: bool = False):\n",
    "        \"\"\"\n",
    "        Executes a list or iterable of SQL queries. \n",
    "        If fetchrows is True, iterates over queries and fetches rows.\n",
    "        If fetchrows is False, tries to use executemany for batch execution.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        queries : list or str\n",
    "            A list of SQL queries to be executed, or a single query as a string.\n",
    "        \n",
    "        params : list, optional\n",
    "            A list of tuples containing parameters for each query. Defaults to None.\n",
    "        \n",
    "        fetchrows : bool, optional\n",
    "            Whether to fetch rows from the queries. Defaults to False (use executemany).\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        results : list\n",
    "            A list of results for each executed query, or None if using executemany.\n",
    "        \"\"\"\n",
    "        if not self.connection:\n",
    "            self.connect()\n",
    "\n",
    "        results = []\n",
    "        with self.connection.cursor() as cur:\n",
    "            if fetchrows:\n",
    "\n",
    "                if isinstance(queries, str):\n",
    "                    # Convert single query to a list of same length as params\n",
    "                    queries = [queries] * len(params)\n",
    "\n",
    "                # Iterate over queries and fetch rows\n",
    "                for idx, query in enumerate(queries):\n",
    "                    query_params = params[idx] if params else None\n",
    "                    cur.execute(query, query_params)\n",
    "                    result = cur.fetchone()\n",
    "                    results.append(result)  # Collect the result of each query\n",
    "            else:\n",
    "\n",
    "                if not isinstance(queries, str):\n",
    "                    # In this case only one query with multiple params can be executed (raise error)\n",
    "                    raise ValueError(\"Multiple queries with multiple params are not supported when using executemany. Set fetchrows=True\")\n",
    "                                \n",
    "                cur.executemany(queries, params)\n",
    "\n",
    "        self.connection.commit()\n",
    "\n",
    "        return results if fetchrows else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
